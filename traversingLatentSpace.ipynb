{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import torch\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "from typing import Tuple\n",
    "\n",
    "# scVI imports\n",
    "import scvi\n",
    "from scvi.dataset import AnnDatasetFromAnnData\n",
    "from scvi.inference import UnsupervisedTrainer\n",
    "from scvi.models.vae import VAE\n",
    "from FactorVAE import FactorVAE, factorTrain\n",
    "\n",
    "show_plot = True\n",
    "test_mode = False\n",
    "n_epochs_all = None\n",
    "save_path = \"data/\"\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "sc.settings.verbosity = 0\n",
    "sc.settings.set_figure_params(dpi=60)\n",
    "\n",
    "%matplotlib inline\n",
    "adata = sc.read_10x_mtx(\n",
    "    os.path.join(\n",
    "        save_path, \"filtered_gene_bc_matrices/hg19/\"\n",
    "    ),  # the directory with the `.mtx` file\n",
    "    var_names=\"gene_symbols\",  # use gene symbols for the variable names (variables-axis index)\n",
    ")\n",
    "adata.var_names_make_unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered out 19024 genes that are detected in less than 3 cells\n"
     ]
    }
   ],
   "source": [
    "def if_not_test_else(x, y):\n",
    "    if not test_mode:\n",
    "        return x\n",
    "    else:\n",
    "        return y\n",
    "\n",
    "min_genes = if_not_test_else(200, 0)\n",
    "min_cells = if_not_test_else(3, 0)\n",
    "sc.settings.verbosity = 2\n",
    "sc.pp.filter_cells(adata, min_genes=min_genes)\n",
    "sc.pp.filter_genes(adata, min_cells=min_cells)\n",
    "sc.pp.filter_cells(adata, min_genes=1)\n",
    "mito_genes = adata.var_names.str.startswith(\"MT-\")\n",
    "adata.obs[\"percent_mito\"] = (\n",
    "    np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1\n",
    ")\n",
    "adata.obs[\"n_counts\"] = adata.X.sum(axis=1).A1\n",
    "adata = adata[adata.obs[\"n_genes\"] < 2500, :]\n",
    "adata = adata[adata.obs[\"percent_mito\"] < 0.05, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalizing by total count per cell\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to set attribute `.obs` of view, copying.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    finished (0:00:00): normalized adata.X and added    'n_counts', counts per cell before normalization (adata.obs)\n",
      "extracting highly variable genes\n",
      "    finished (0:00:01)\n",
      "regressing out ['n_counts', 'percent_mito']\n",
      "    sparse input is densified and may lead to high memory use\n",
      "    finished (0:00:16)\n",
      "1838 highly variable genes\n"
     ]
    }
   ],
   "source": [
    "adata_original = adata.copy()\n",
    "\n",
    "sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4)\n",
    "sc.pp.log1p(adata)\n",
    "\n",
    "\n",
    "min_mean = if_not_test_else(0.0125, -np.inf)\n",
    "max_mean = if_not_test_else(3, np.inf)\n",
    "min_disp = if_not_test_else(0.5, -np.inf)\n",
    "max_disp = if_not_test_else(None, np.inf)\n",
    "\n",
    "sc.pp.highly_variable_genes(\n",
    "    adata,\n",
    "    min_mean=min_mean,\n",
    "    max_mean=max_mean,\n",
    "    min_disp=min_disp,\n",
    "    max_disp=max_disp\n",
    ")\n",
    "\n",
    "highly_variable_genes = adata.var[\"highly_variable\"]\n",
    "adata = adata[:, highly_variable_genes]\n",
    "\n",
    "adata.raw = adata\n",
    "\n",
    "sc.pp.regress_out(adata, [\"n_counts\", \"percent_mito\"])\n",
    "sc.pp.scale(adata, max_value=10)\n",
    "\n",
    "# Also filter the original adata genes\n",
    "adata_original = adata_original[:, highly_variable_genes]\n",
    "print(\"{} highly variable genes\".format(highly_variable_genes.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scvi_latent(\n",
    "    adata: sc.AnnData,\n",
    "    n_latent: int = 5,\n",
    "    n_epochs: int = 100,\n",
    "    lr: float = 1e-3,\n",
    "    use_batches: bool = False,\n",
    "    use_cuda: bool = False,\n",
    ") -> Tuple[scvi.inference.Posterior, np.ndarray]:\n",
    "    \"\"\"Train and return a scVI model and sample a latent space\n",
    "\n",
    "    :param adata: sc.AnnData object non-normalized\n",
    "    :param n_latent: dimension of the latent space\n",
    "    :param n_epochs: number of training epochs\n",
    "    :param lr: learning rate\n",
    "    :param use_batches\n",
    "    :param use_cuda\n",
    "    :return: (scvi.Posterior, latent_space)\n",
    "    \"\"\"\n",
    "    # Convert easily to scvi dataset\n",
    "    scviDataset = AnnDatasetFromAnnData(adata)\n",
    "\n",
    "    # Train a model\n",
    "    vae = FactorVAE(\n",
    "        scviDataset.nb_genes,\n",
    "        n_batch=scviDataset.n_batches * use_batches,\n",
    "        n_latent=n_latent,\n",
    "    )\n",
    "    trainer = factorTrain(vae, scviDataset, train_size=1.0, use_cuda=use_cuda)\n",
    "    trainer.train(n_epochs=n_epochs, lr=lr)\n",
    "    ####\n",
    "\n",
    "    # Extract latent space\n",
    "    posterior = trainer.create_posterior(\n",
    "        trainer.model, scviDataset, indices=np.arange(len(scviDataset))\n",
    "    ).sequential()\n",
    "\n",
    "    latent, _, _ = posterior.get_latent()\n",
    "\n",
    "    return posterior, latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2638, 1838)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-06 00:03:24,847] INFO - scvi.dataset.anndataset | Dense size under 1Gb, casting to dense format (np.ndarray).\n",
      "[2020-05-06 00:03:24,872] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-05-06 00:03:24,874] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-05-06 00:03:24,954] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-05-06 00:03:24,976] INFO - scvi.dataset.dataset | Downsampled from 2638 to 2638 cells\n",
      "model n latent 10\n",
      "n epoch is None\n",
      "[2020-05-06 00:03:25,015] INFO - scvi.inference.inference | KL warmup phase exceeds overall training phaseIf your applications rely on the posterior quality, consider training for more epochs or reducing the kl warmup.\n",
      "[2020-05-06 00:03:25,016] INFO - scvi.inference.inference | KL warmup for 400 epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b8520764c741799804540a94ae51d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training', max=50.0, style=ProgressStyle(description_widtâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE loss 1957.7655029296875, Discriminator loss 0.0002627553476486355\n",
      "VAE loss 2009.5103759765625, Discriminator loss 0.00026275479467585683\n",
      "VAE loss 1810.6273193359375, Discriminator loss 0.00026275470736436546\n",
      "VAE loss 1770.005126953125, Discriminator loss 0.00026275429991073906\n",
      "VAE loss 1891.4742431640625, Discriminator loss 0.0002627542708069086\n",
      "VAE loss 1786.8343505859375, Discriminator loss 0.00026275409618392587\n",
      "VAE loss 1681.4393310546875, Discriminator loss 0.00026275429991073906\n",
      "VAE loss 1808.1409912109375, Discriminator loss 0.0002627532812766731\n",
      "VAE loss 1747.5904541015625, Discriminator loss 0.0002627513313200325\n",
      "VAE loss 1743.70947265625, Discriminator loss 0.00026275176787748933\n",
      "VAE loss 1683.400634765625, Discriminator loss 0.0002627488865982741\n",
      "VAE loss 1625.339599609375, Discriminator loss 0.00026274865376763046\n",
      "VAE loss 1547.3431396484375, Discriminator loss 0.00026274833362549543\n",
      "VAE loss 1554.2578125, Discriminator loss 0.0002627440553624183\n",
      "VAE loss 1566.468994140625, Discriminator loss 0.00026274335687048733\n",
      "VAE loss 1555.7972412109375, Discriminator loss 0.00026274219271726906\n",
      "VAE loss 1510.0350341796875, Discriminator loss 0.0002627362555358559\n",
      "VAE loss 1336.623779296875, Discriminator loss 0.00026273931143805385\n",
      "VAE loss 1479.981689453125, Discriminator loss 0.0002627348294481635\n",
      "VAE loss 1398.5648193359375, Discriminator loss 0.00026272833929397166\n",
      "VAE loss 1413.054931640625, Discriminator loss 0.0002627322101034224\n",
      "VAE loss 1374.56787109375, Discriminator loss 0.0002627274370752275\n",
      "VAE loss 1291.0645751953125, Discriminator loss 0.00026271885144524276\n",
      "VAE loss 1357.1929931640625, Discriminator loss 0.00026271151727996767\n",
      "VAE loss 1239.02392578125, Discriminator loss 0.00026271166279911995\n",
      "VAE loss 1307.0579833984375, Discriminator loss 0.00026270729722455144\n",
      "VAE loss 1283.871826171875, Discriminator loss 0.00026270514354109764\n",
      "VAE loss 1231.71923828125, Discriminator loss 0.0002626919304020703\n",
      "VAE loss 1219.5859375, Discriminator loss 0.0002626942878123373\n",
      "VAE loss 1236.7515869140625, Discriminator loss 0.000262684712652117\n",
      "VAE loss 1240.5360107421875, Discriminator loss 0.00026267554494552314\n",
      "VAE loss 1221.8770751953125, Discriminator loss 0.0002626791538204998\n",
      "VAE loss 1216.9892578125, Discriminator loss 0.0002626636705826968\n",
      "VAE loss 1171.904296875, Discriminator loss 0.00026265476481057703\n",
      "VAE loss 1194.5628662109375, Discriminator loss 0.0002626474015414715\n",
      "VAE loss 1139.0164794921875, Discriminator loss 0.0002626370405778289\n",
      "VAE loss 1118.2347412109375, Discriminator loss 0.00026263901963829994\n",
      "VAE loss 1046.7159423828125, Discriminator loss 0.00026264754706062376\n",
      "VAE loss 1072.885986328125, Discriminator loss 0.00026263465406373143\n",
      "VAE loss 1115.0181884765625, Discriminator loss 0.00026262275059707463\n",
      "VAE loss 1114.689208984375, Discriminator loss 0.0002625985362101346\n",
      "VAE loss 1113.23046875, Discriminator loss 0.00026259530568495393\n",
      "VAE loss 1018.5835571289062, Discriminator loss 0.00026257988065481186\n",
      "VAE loss 971.7501220703125, Discriminator loss 0.0002625853521749377\n",
      "VAE loss 994.9754638671875, Discriminator loss 0.00026255182456225157\n",
      "VAE loss 1007.119384765625, Discriminator loss 0.0002625593333505094\n",
      "VAE loss 911.536865234375, Discriminator loss 0.0002625277847982943\n",
      "VAE loss 979.90771484375, Discriminator loss 0.00026248471112921834\n",
      "VAE loss 954.3727416992188, Discriminator loss 0.0002625137276481837\n",
      "VAE loss 930.5596313476562, Discriminator loss 0.000262477871729061\n",
      "VAE loss 975.2279052734375, Discriminator loss 0.0002624474873300642\n",
      "VAE loss 907.4320678710938, Discriminator loss 0.0002624732442200184\n",
      "VAE loss 883.1613159179688, Discriminator loss 0.00026241736486554146\n",
      "VAE loss 837.9988403320312, Discriminator loss 0.00026240423903800547\n",
      "VAE loss 854.7034912109375, Discriminator loss 0.00026241366867907345\n",
      "VAE loss 897.2626342773438, Discriminator loss 0.00026245697517879307\n",
      "VAE loss 937.5886840820312, Discriminator loss 0.0002623220207169652\n",
      "VAE loss 785.18359375, Discriminator loss 0.00026227065245620906\n",
      "VAE loss 801.87744140625, Discriminator loss 0.0002622642205096781\n",
      "VAE loss 830.2098388671875, Discriminator loss 0.00026229573995806277\n",
      "VAE loss 792.2083740234375, Discriminator loss 0.0002622352330945432\n",
      "VAE loss 783.3002319335938, Discriminator loss 0.0002621508901938796\n",
      "VAE loss 808.313232421875, Discriminator loss 0.0002621488238219172\n",
      "VAE loss 779.3973999023438, Discriminator loss 0.00026208581402897835\n",
      "VAE loss 774.6666259765625, Discriminator loss 0.00026206354959867895\n",
      "VAE loss 732.1214599609375, Discriminator loss 0.00026200470165349543\n",
      "VAE loss 744.11328125, Discriminator loss 0.00026204410823993385\n",
      "VAE loss 707.10302734375, Discriminator loss 0.0002619755105115473\n",
      "VAE loss 706.124267578125, Discriminator loss 0.00026187769253738225\n",
      "VAE loss 681.4195556640625, Discriminator loss 0.00026181238354183733\n",
      "VAE loss 703.6165771484375, Discriminator loss 0.0002618285652715713\n",
      "VAE loss 705.972412109375, Discriminator loss 0.0002616682031657547\n",
      "VAE loss 713.4075927734375, Discriminator loss 0.0002617066784296185\n",
      "VAE loss 696.1464233398438, Discriminator loss 0.00026153260841965675\n",
      "VAE loss 698.078369140625, Discriminator loss 0.0002615370904095471\n",
      "VAE loss 666.525390625, Discriminator loss 0.0002615057455841452\n",
      "VAE loss 669.4730834960938, Discriminator loss 0.0002612416574265808\n",
      "VAE loss 644.1617431640625, Discriminator loss 0.00026125661679543555\n",
      "VAE loss 632.8734130859375, Discriminator loss 0.0002612585376482457\n",
      "VAE loss 618.5183715820312, Discriminator loss 0.0002610469236969948\n",
      "VAE loss 671.8413696289062, Discriminator loss 0.00026092282496392727\n",
      "VAE loss 658.8826293945312, Discriminator loss 0.00026071880711242557\n",
      "VAE loss 672.716552734375, Discriminator loss 0.0002607125788927078\n",
      "VAE loss 614.8364868164062, Discriminator loss 0.00026072017499245703\n",
      "VAE loss 613.51953125, Discriminator loss 0.0002605180779937655\n",
      "VAE loss 614.0703125, Discriminator loss 0.00026032686582766473\n",
      "VAE loss 611.5226440429688, Discriminator loss 0.00026021525263786316\n",
      "VAE loss 602.1316528320312, Discriminator loss 0.0002603289613034576\n",
      "VAE loss 612.8499755859375, Discriminator loss 0.00025963757070712745\n",
      "VAE loss 642.4308471679688, Discriminator loss 0.00026005375548265874\n",
      "VAE loss 630.0176391601562, Discriminator loss 0.0002592756354715675\n",
      "VAE loss 559.5180053710938, Discriminator loss 0.00025951408315449953\n",
      "VAE loss 610.7822265625, Discriminator loss 0.0002590201038401574\n",
      "VAE loss 609.2647094726562, Discriminator loss 0.00025881596957333386\n",
      "VAE loss 601.2515869140625, Discriminator loss 0.00025862379698082805\n",
      "VAE loss 602.3937377929688, Discriminator loss 0.00025856218417175114\n",
      "VAE loss 624.4180908203125, Discriminator loss 0.0002584381145425141\n",
      "VAE loss 582.2401733398438, Discriminator loss 0.0002580938453320414\n",
      "VAE loss 602.2854614257812, Discriminator loss 0.00025824332260526717\n",
      "VAE loss 562.119384765625, Discriminator loss 0.0002575790567789227\n",
      "VAE loss 566.8820190429688, Discriminator loss 0.0002573672973085195\n",
      "VAE loss 558.9756469726562, Discriminator loss 0.0002577069972176105\n",
      "VAE loss 597.9454345703125, Discriminator loss 0.00025694523355923593\n",
      "VAE loss 617.266845703125, Discriminator loss 0.00025638489751145244\n",
      "VAE loss 604.810302734375, Discriminator loss 0.00025582974194549024\n",
      "VAE loss 599.1592407226562, Discriminator loss 0.0002562512527219951\n",
      "VAE loss 589.8090209960938, Discriminator loss 0.0002558440901339054\n",
      "VAE loss 578.583740234375, Discriminator loss 0.0002559421700425446\n",
      "VAE loss 619.426513671875, Discriminator loss 0.0002558229607529938\n",
      "VAE loss 600.6439208984375, Discriminator loss 0.0002546437317505479\n",
      "VAE loss 579.8873901367188, Discriminator loss 0.0002541563881095499\n",
      "VAE loss 558.4518432617188, Discriminator loss 0.00025389541406184435\n",
      "VAE loss 583.1761474609375, Discriminator loss 0.0002541649737395346\n",
      "VAE loss 577.923828125, Discriminator loss 0.0002532428770791739\n",
      "VAE loss 613.485595703125, Discriminator loss 0.000252410740358755\n",
      "VAE loss 589.52783203125, Discriminator loss 0.0002518190012779087\n",
      "VAE loss 591.9410400390625, Discriminator loss 0.00025135939358733594\n",
      "VAE loss 609.3944702148438, Discriminator loss 0.0002507663157302886\n",
      "VAE loss 580.3009643554688, Discriminator loss 0.00025061063934117556\n",
      "VAE loss 585.2457275390625, Discriminator loss 0.0002513908257242292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE loss 574.713134765625, Discriminator loss 0.0002495650842320174\n",
      "VAE loss 567.2161865234375, Discriminator loss 0.0002494301297701895\n",
      "VAE loss 575.166259765625, Discriminator loss 0.00024783617118373513\n",
      "VAE loss 567.6339721679688, Discriminator loss 0.00024820014368742704\n",
      "VAE loss 590.3461303710938, Discriminator loss 0.00024802295956760645\n",
      "VAE loss 552.521484375, Discriminator loss 0.0002449938328936696\n",
      "VAE loss 585.97119140625, Discriminator loss 0.0002454514615237713\n",
      "VAE loss 576.8316040039062, Discriminator loss 0.00024462476721964777\n",
      "VAE loss 558.3467407226562, Discriminator loss 0.00024309309083037078\n",
      "VAE loss 593.5245971679688, Discriminator loss 0.00024366365687455982\n",
      "VAE loss 591.5225219726562, Discriminator loss 0.00024284992832690477\n",
      "VAE loss 589.1961059570312, Discriminator loss 0.00024415142252109945\n",
      "VAE loss 585.6823120117188, Discriminator loss 0.00024051462241914123\n",
      "VAE loss 569.3109130859375, Discriminator loss 0.00024397452943958342\n",
      "VAE loss 584.8399658203125, Discriminator loss 0.00023997259268071502\n",
      "VAE loss 578.6905517578125, Discriminator loss 0.0002389512228546664\n",
      "VAE loss 560.9210205078125, Discriminator loss 0.00024030351778492332\n",
      "VAE loss 579.9215087890625, Discriminator loss 0.0002368806308368221\n",
      "VAE loss 581.540771484375, Discriminator loss 0.0002386697451584041\n",
      "VAE loss 553.3770141601562, Discriminator loss 0.00023332543787546456\n",
      "VAE loss 567.9200439453125, Discriminator loss 0.00023610651260241866\n",
      "VAE loss 569.3522338867188, Discriminator loss 0.0002327487018192187\n",
      "VAE loss 612.804931640625, Discriminator loss 0.00023125691222958267\n",
      "VAE loss 589.4348754882812, Discriminator loss 0.00023026354028843343\n",
      "VAE loss 560.2322998046875, Discriminator loss 0.00023380519996862859\n",
      "VAE loss 574.1806640625, Discriminator loss 0.00022791519586462528\n",
      "VAE loss 578.6990356445312, Discriminator loss 0.0002277233434142545\n",
      "VAE loss 590.6799926757812, Discriminator loss 0.00022586039267480373\n",
      "VAE loss 577.1893920898438, Discriminator loss 0.00022636068752035499\n",
      "VAE loss 576.5866088867188, Discriminator loss 0.00022680050460621715\n",
      "VAE loss 558.384033203125, Discriminator loss 0.00022781889128964394\n",
      "VAE loss 561.3330078125, Discriminator loss 0.00022153226018417627\n",
      "VAE loss 558.9373168945312, Discriminator loss 0.00022196327336132526\n",
      "VAE loss 603.202880859375, Discriminator loss 0.00022414285922423005\n",
      "VAE loss 620.0676879882812, Discriminator loss 0.00021535549603868276\n",
      "VAE loss 566.446533203125, Discriminator loss 0.00021244179515633732\n",
      "VAE loss 581.7448120117188, Discriminator loss 0.00021861858840566128\n",
      "VAE loss 551.8682861328125, Discriminator loss 0.00021531956735998392\n",
      "VAE loss 561.263427734375, Discriminator loss 0.00021744171681348234\n",
      "VAE loss 553.5562744140625, Discriminator loss 0.0002095472882501781\n",
      "VAE loss 596.0797729492188, Discriminator loss 0.00020532344933599234\n",
      "VAE loss 564.9270629882812, Discriminator loss 0.00021142307377886027\n",
      "VAE loss 569.4746704101562, Discriminator loss 0.00020650085934903473\n",
      "VAE loss 608.6454467773438, Discriminator loss 0.00020697896252386272\n",
      "VAE loss 600.3916015625, Discriminator loss 0.00020150383352302015\n",
      "VAE loss 563.637451171875, Discriminator loss 0.00020934818894602358\n",
      "VAE loss 571.4937744140625, Discriminator loss 0.0002009218733292073\n",
      "VAE loss 548.6642456054688, Discriminator loss 0.00019088531553279608\n",
      "VAE loss 564.1035766601562, Discriminator loss 0.0002012426994042471\n",
      "VAE loss 581.15869140625, Discriminator loss 0.00019630602037068456\n",
      "VAE loss 562.2665405273438, Discriminator loss 0.00019191522733308375\n",
      "VAE loss 562.573486328125, Discriminator loss 0.00018882699077948928\n",
      "VAE loss 581.181396484375, Discriminator loss 0.00020266399951651692\n",
      "VAE loss 570.6045532226562, Discriminator loss 0.0001973071921383962\n",
      "VAE loss 589.6078491210938, Discriminator loss 0.00019115590839646757\n",
      "VAE loss 569.8185424804688, Discriminator loss 0.00017637843848206103\n",
      "VAE loss 568.86279296875, Discriminator loss 0.0001813142152968794\n",
      "VAE loss 574.1409912109375, Discriminator loss 0.0001806259824661538\n",
      "VAE loss 566.375, Discriminator loss 0.0001804274070309475\n",
      "VAE loss 561.6941528320312, Discriminator loss 0.0001715124526526779\n",
      "VAE loss 551.4585571289062, Discriminator loss 0.0001830714027164504\n",
      "VAE loss 560.5270385742188, Discriminator loss 0.0001758322905516252\n",
      "VAE loss 571.2528686523438, Discriminator loss 0.00017280803876928985\n",
      "VAE loss 564.9103393554688, Discriminator loss 0.00017854335601441562\n",
      "VAE loss 572.0868530273438, Discriminator loss 0.00017294270219281316\n",
      "VAE loss 578.6898803710938, Discriminator loss 0.00017231868696399033\n",
      "VAE loss 566.847900390625, Discriminator loss 0.0001761859020916745\n",
      "VAE loss 589.9563598632812, Discriminator loss 0.00017209934594575316\n",
      "VAE loss 619.5009155273438, Discriminator loss 0.000170249622897245\n",
      "VAE loss 575.5330200195312, Discriminator loss 0.0001637026434764266\n",
      "VAE loss 585.6015014648438, Discriminator loss 0.0001712199446046725\n",
      "VAE loss 547.8443603515625, Discriminator loss 0.00015786269796080887\n",
      "VAE loss 567.3873291015625, Discriminator loss 0.0001678606786299497\n",
      "VAE loss 564.1251831054688, Discriminator loss 0.0001632936327951029\n",
      "VAE loss 573.0357055664062, Discriminator loss 0.00014458518126048148\n",
      "VAE loss 528.2747192382812, Discriminator loss 0.00015764113049954176\n",
      "VAE loss 574.4024047851562, Discriminator loss 0.00015417386020999402\n",
      "VAE loss 553.2429809570312, Discriminator loss 0.0001512545277364552\n",
      "VAE loss 554.6734008789062, Discriminator loss 0.00014573622320313007\n",
      "VAE loss 572.8209228515625, Discriminator loss 0.00015588290989398956\n",
      "VAE loss 543.7083740234375, Discriminator loss 0.00014594987442251295\n",
      "VAE loss 561.0095825195312, Discriminator loss 0.0001475366298109293\n",
      "VAE loss 559.301025390625, Discriminator loss 0.000147820872371085\n",
      "VAE loss 587.9342041015625, Discriminator loss 0.00013585321721620858\n",
      "VAE loss 564.8807373046875, Discriminator loss 0.00013323240273166448\n",
      "VAE loss 586.4931640625, Discriminator loss 0.00013319595018401742\n",
      "VAE loss 553.172119140625, Discriminator loss 0.00013538818166125566\n",
      "VAE loss 563.241455078125, Discriminator loss 0.00014701909094583243\n",
      "VAE loss 581.7325439453125, Discriminator loss 0.00013593373296316713\n",
      "VAE loss 526.7553100585938, Discriminator loss 0.00014121485583018512\n",
      "VAE loss 543.9077758789062, Discriminator loss 0.0001388987002428621\n",
      "VAE loss 565.4717407226562, Discriminator loss 0.0001394025020999834\n",
      "VAE loss 578.2630615234375, Discriminator loss 0.00015156870358623564\n",
      "VAE loss 553.3208618164062, Discriminator loss 0.0001434956066077575\n",
      "VAE loss 568.2134399414062, Discriminator loss 0.000130067317513749\n",
      "VAE loss 559.3327026367188, Discriminator loss 0.00012900367437396199\n",
      "VAE loss 549.0286254882812, Discriminator loss 0.00011762602662201971\n",
      "VAE loss 545.98876953125, Discriminator loss 0.00013059409684501588\n",
      "VAE loss 551.1802978515625, Discriminator loss 0.00013910308189224452\n",
      "VAE loss 577.788330078125, Discriminator loss 0.00014450242451857775\n",
      "VAE loss 537.3309326171875, Discriminator loss 0.00013837448204867542\n",
      "VAE loss 551.6983642578125, Discriminator loss 0.0001323988108197227\n",
      "VAE loss 545.9290161132812, Discriminator loss 0.00014361632929649204\n",
      "VAE loss 548.7556762695312, Discriminator loss 0.00012591657286975533\n",
      "VAE loss 537.6199951171875, Discriminator loss 0.00014136492973193526\n",
      "VAE loss 572.4503173828125, Discriminator loss 0.00010827124788193032\n",
      "VAE loss 612.9564208984375, Discriminator loss 0.00010307831689715385\n",
      "VAE loss 559.5789794921875, Discriminator loss 0.00013504037633538246\n",
      "VAE loss 545.1505737304688, Discriminator loss 0.00012551643885672092\n",
      "VAE loss 559.7603759765625, Discriminator loss 0.0001153697376139462\n",
      "VAE loss 560.3247680664062, Discriminator loss 0.00013262119318824261\n",
      "VAE loss 560.0592041015625, Discriminator loss 0.00011797894694609568\n",
      "VAE loss 548.88232421875, Discriminator loss 0.0001273629895877093\n",
      "VAE loss 584.6629028320312, Discriminator loss 9.777195373317227e-05\n",
      "VAE loss 543.4026489257812, Discriminator loss 0.00011899564560735598\n",
      "VAE loss 552.146240234375, Discriminator loss 0.00014767491666134447\n",
      "VAE loss 540.26806640625, Discriminator loss 0.00013178637891542166\n",
      "VAE loss 560.6497802734375, Discriminator loss 0.0001221112470375374\n",
      "VAE loss 561.875, Discriminator loss 9.646553371567279e-05\n",
      "VAE loss 550.2620849609375, Discriminator loss 0.00013172916078474373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE loss 583.220458984375, Discriminator loss 0.00011266903311479837\n",
      "VAE loss 579.1939697265625, Discriminator loss 0.0001043129523168318\n",
      "VAE loss 539.300537109375, Discriminator loss 0.00012342676927801222\n",
      "VAE loss 557.5191650390625, Discriminator loss 0.00011438239744165912\n",
      "VAE loss 531.4598388671875, Discriminator loss 0.00010197191295446828\n",
      "VAE loss 598.7306518554688, Discriminator loss 0.00011274001735728234\n",
      "VAE loss 535.6848754882812, Discriminator loss 9.567043889546767e-05\n",
      "VAE loss 576.1296997070312, Discriminator loss 9.562668128637597e-05\n",
      "VAE loss 568.4287719726562, Discriminator loss 8.706175140105188e-05\n",
      "VAE loss 553.7638549804688, Discriminator loss 8.796337351668626e-05\n",
      "VAE loss 580.2137451171875, Discriminator loss 0.00013659830437973142\n",
      "VAE loss 520.5426635742188, Discriminator loss 0.00010494728485355154\n",
      "VAE loss 566.15771484375, Discriminator loss 0.00011224539775867015\n",
      "VAE loss 546.9555053710938, Discriminator loss 0.00010681983985705301\n",
      "VAE loss 554.3056640625, Discriminator loss 0.00010484334052307531\n",
      "VAE loss 535.8668212890625, Discriminator loss 0.00010587939323158935\n",
      "VAE loss 539.5257568359375, Discriminator loss 0.00011676570284180343\n",
      "VAE loss 566.1618041992188, Discriminator loss 8.604849426774308e-05\n",
      "VAE loss 600.1845092773438, Discriminator loss 0.00010314059909433126\n",
      "VAE loss 554.8499755859375, Discriminator loss 0.00010109504364663735\n",
      "VAE loss 555.5640869140625, Discriminator loss 9.453578240936622e-05\n",
      "VAE loss 544.5478515625, Discriminator loss 0.00011014513438567519\n",
      "VAE loss 545.8206787109375, Discriminator loss 9.765381400939077e-05\n",
      "VAE loss 541.1817016601562, Discriminator loss 9.720947127789259e-05\n",
      "VAE loss 539.322509765625, Discriminator loss 0.00010314739483874291\n",
      "VAE loss 562.6376342773438, Discriminator loss 0.00011278592864982784\n",
      "VAE loss 608.7504272460938, Discriminator loss 0.00015878603153396398\n",
      "VAE loss 543.356689453125, Discriminator loss 0.00011468939192127436\n",
      "VAE loss 557.97802734375, Discriminator loss 0.00011039252422051504\n",
      "VAE loss 592.81787109375, Discriminator loss 0.0001082358430721797\n",
      "VAE loss 566.6681518554688, Discriminator loss 0.00014041218673810363\n",
      "VAE loss 573.3491821289062, Discriminator loss 9.056537237484008e-05\n",
      "VAE loss 548.772705078125, Discriminator loss 8.156934200087562e-05\n",
      "VAE loss 572.30859375, Discriminator loss 0.00010593237675493583\n",
      "VAE loss 565.8775634765625, Discriminator loss 9.443570888834074e-05\n",
      "VAE loss 522.3603515625, Discriminator loss 0.00010479209595359862\n",
      "VAE loss 574.7474975585938, Discriminator loss 0.00013335076801013201\n",
      "VAE loss 546.3750610351562, Discriminator loss 8.98852085811086e-05\n",
      "VAE loss 533.613037109375, Discriminator loss 0.000111391294922214\n",
      "VAE loss 545.08251953125, Discriminator loss 0.00010163307888433337\n",
      "VAE loss 570.6738891601562, Discriminator loss 0.00011281252227490768\n",
      "VAE loss 539.5067138671875, Discriminator loss 0.00013445765944197774\n",
      "VAE loss 536.299560546875, Discriminator loss 0.00010593480692477897\n",
      "VAE loss 536.7581787109375, Discriminator loss 0.0001413562276866287\n",
      "VAE loss 593.9700317382812, Discriminator loss 0.00012199427146697417\n",
      "VAE loss 543.17724609375, Discriminator loss 0.00013144779950380325\n",
      "VAE loss 546.89404296875, Discriminator loss 9.19549202080816e-05\n",
      "VAE loss 536.6985473632812, Discriminator loss 0.00011203202302567661\n",
      "VAE loss 540.72509765625, Discriminator loss 0.00011572491348488256\n",
      "VAE loss 529.1959838867188, Discriminator loss 9.903134196065366e-05\n",
      "VAE loss 565.4290161132812, Discriminator loss 0.00010699236736400053\n",
      "VAE loss 592.3651733398438, Discriminator loss 0.00011278983583906665\n",
      "VAE loss 548.572509765625, Discriminator loss 0.00010800577729241922\n",
      "VAE loss 595.3329467773438, Discriminator loss 8.984014129964635e-05\n",
      "VAE loss 559.3712768554688, Discriminator loss 0.0001272790104849264\n",
      "VAE loss 528.7638549804688, Discriminator loss 0.00010429760004626587\n",
      "VAE loss 566.9788208007812, Discriminator loss 0.00015174312284216285\n",
      "VAE loss 556.3530883789062, Discriminator loss 0.0001126579154515639\n",
      "VAE loss 563.2020263671875, Discriminator loss 9.901332668960094e-05\n",
      "VAE loss 551.0613403320312, Discriminator loss 9.510106610832736e-05\n",
      "VAE loss 554.89990234375, Discriminator loss 0.0001183519561891444\n",
      "VAE loss 559.9218139648438, Discriminator loss 0.00011718802124960348\n",
      "VAE loss 533.5048217773438, Discriminator loss 0.00010071320866700262\n",
      "VAE loss 555.6971435546875, Discriminator loss 0.00012188685650471598\n",
      "VAE loss 566.249755859375, Discriminator loss 0.00011622440069913864\n",
      "VAE loss 545.6948852539062, Discriminator loss 0.00011053161870222539\n",
      "VAE loss 524.208251953125, Discriminator loss 9.275115735363215e-05\n",
      "VAE loss 527.8153076171875, Discriminator loss 9.408238111063838e-05\n",
      "VAE loss 561.6619873046875, Discriminator loss 0.00010467176616657525\n",
      "VAE loss 536.5975341796875, Discriminator loss 0.0001137369908974506\n",
      "VAE loss 531.2111206054688, Discriminator loss 0.00011890039604622871\n",
      "VAE loss 559.39306640625, Discriminator loss 8.993931260192767e-05\n",
      "VAE loss 555.595458984375, Discriminator loss 0.00012155777221778408\n",
      "VAE loss 551.8277587890625, Discriminator loss 0.00010064955858979374\n",
      "VAE loss 603.9828491210938, Discriminator loss 0.0001137005165219307\n",
      "VAE loss 530.1629638671875, Discriminator loss 0.00012161269114585593\n",
      "VAE loss 539.984130859375, Discriminator loss 0.00012202723883092403\n",
      "VAE loss 545.2901000976562, Discriminator loss 0.00010336753621231765\n",
      "VAE loss 584.62451171875, Discriminator loss 0.0001152309196186252\n",
      "VAE loss 566.7916259765625, Discriminator loss 0.00010047313844552264\n",
      "VAE loss 532.0570068359375, Discriminator loss 9.108692029258236e-05\n",
      "VAE loss 588.254150390625, Discriminator loss 0.0001381490583298728\n",
      "VAE loss 542.0023193359375, Discriminator loss 0.0001034110682667233\n",
      "VAE loss 560.3418579101562, Discriminator loss 6.59524230286479e-05\n",
      "VAE loss 526.9357299804688, Discriminator loss 0.00012029478239128366\n",
      "VAE loss 552.2093505859375, Discriminator loss 0.00012659271305892617\n",
      "VAE loss 557.8641357421875, Discriminator loss 0.00013643490092363209\n",
      "VAE loss 541.9832763671875, Discriminator loss 0.00011024464038200676\n",
      "VAE loss 571.5731811523438, Discriminator loss 0.00013959931675344706\n",
      "VAE loss 537.7237548828125, Discriminator loss 0.00010867910896195099\n",
      "VAE loss 542.8600463867188, Discriminator loss 0.00014894116611685604\n",
      "VAE loss 539.9071655273438, Discriminator loss 0.0001371001562802121\n",
      "VAE loss 551.2245483398438, Discriminator loss 0.00010882021160796285\n",
      "VAE loss 581.0276489257812, Discriminator loss 0.00010344483598601073\n",
      "VAE loss 534.7618408203125, Discriminator loss 0.00010143289546249434\n",
      "VAE loss 593.314697265625, Discriminator loss 0.00012323647388257086\n",
      "VAE loss 534.3865356445312, Discriminator loss 0.00013307113840710372\n",
      "VAE loss 627.5404663085938, Discriminator loss 0.0001088896970031783\n",
      "VAE loss 537.2382202148438, Discriminator loss 0.00011788879055529833\n",
      "VAE loss 537.9553833007812, Discriminator loss 0.00011714677384588867\n",
      "VAE loss 579.447998046875, Discriminator loss 0.0001277136179851368\n",
      "VAE loss 560.2473754882812, Discriminator loss 0.00015108942170627415\n",
      "VAE loss 544.1989135742188, Discriminator loss 0.00010288810881320387\n",
      "VAE loss 537.5751953125, Discriminator loss 0.00014471824397332966\n",
      "VAE loss 547.5359497070312, Discriminator loss 0.00011088720202678815\n",
      "VAE loss 533.69482421875, Discriminator loss 0.0001112359605031088\n",
      "VAE loss 544.7369995117188, Discriminator loss 0.0001029580962494947\n",
      "VAE loss 540.4898071289062, Discriminator loss 0.00013521386426873505\n",
      "VAE loss 535.856689453125, Discriminator loss 0.00011751579586416483\n",
      "VAE loss 574.6656494140625, Discriminator loss 9.985910583054647e-05\n",
      "VAE loss 574.4008178710938, Discriminator loss 0.00010058126645162702\n",
      "VAE loss 538.8738403320312, Discriminator loss 0.00012531926040537655\n",
      "VAE loss 609.8058471679688, Discriminator loss 0.00010797937284223735\n",
      "VAE loss 527.4221801757812, Discriminator loss 0.000135152178700082\n",
      "VAE loss 529.8496704101562, Discriminator loss 0.00013527434202842414\n",
      "VAE loss 545.2947998046875, Discriminator loss 0.00011692298721754923\n",
      "VAE loss 520.3463134765625, Discriminator loss 0.00012768928718287498\n",
      "VAE loss 504.42431640625, Discriminator loss 0.000146964899613522\n",
      "VAE loss 558.6887817382812, Discriminator loss 0.0001344093616353348\n",
      "VAE loss 559.851806640625, Discriminator loss 0.00010928380652330816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE loss 537.3806762695312, Discriminator loss 0.00011744794755941257\n",
      "VAE loss 563.9815063476562, Discriminator loss 0.00010494577145436779\n",
      "VAE loss 535.791748046875, Discriminator loss 0.00012101868924219161\n",
      "VAE loss 532.4638671875, Discriminator loss 9.802704880712554e-05\n",
      "VAE loss 533.1943359375, Discriminator loss 0.00013857291196472943\n",
      "VAE loss 564.7763671875, Discriminator loss 0.00015773269115015864\n",
      "VAE loss 549.2028198242188, Discriminator loss 8.943715511122718e-05\n",
      "VAE loss 519.8309326171875, Discriminator loss 0.00013701307761948556\n",
      "VAE loss 555.1935424804688, Discriminator loss 0.00011603865277720615\n",
      "VAE loss 546.5712280273438, Discriminator loss 0.00012491423694882542\n",
      "VAE loss 521.141845703125, Discriminator loss 0.00012606647214852273\n",
      "VAE loss 579.8267211914062, Discriminator loss 0.00011496017395984381\n",
      "VAE loss 549.608642578125, Discriminator loss 0.00015829024778213352\n",
      "VAE loss 541.4352416992188, Discriminator loss 0.00011038866068702191\n",
      "VAE loss 540.7977294921875, Discriminator loss 0.00012310108286328614\n",
      "VAE loss 547.9314575195312, Discriminator loss 0.00014308522804640234\n",
      "VAE loss 575.825927734375, Discriminator loss 9.644679812481627e-05\n",
      "VAE loss 557.93017578125, Discriminator loss 0.00011962496500927955\n",
      "VAE loss 527.4144897460938, Discriminator loss 0.00012101273750886321\n",
      "VAE loss 538.401123046875, Discriminator loss 0.0001340725866612047\n",
      "VAE loss 522.4369506835938, Discriminator loss 0.0001258299598703161\n",
      "VAE loss 532.4778442382812, Discriminator loss 0.00013929963461123407\n",
      "VAE loss 517.27099609375, Discriminator loss 0.0001318007562076673\n",
      "VAE loss 549.6224365234375, Discriminator loss 0.0001376322325086221\n",
      "VAE loss 535.454345703125, Discriminator loss 0.00013674925139639527\n",
      "VAE loss 527.189697265625, Discriminator loss 0.00012425583554431796\n",
      "VAE loss 541.2824096679688, Discriminator loss 9.899689757730812e-05\n",
      "VAE loss 552.2335815429688, Discriminator loss 0.00013338382996153086\n",
      "VAE loss 535.1085815429688, Discriminator loss 0.00014215335249900818\n",
      "VAE loss 522.061279296875, Discriminator loss 0.00010770143853733316\n",
      "VAE loss 555.1910400390625, Discriminator loss 0.00012968042574357241\n",
      "VAE loss 588.471923828125, Discriminator loss 0.00012316650827415287\n",
      "VAE loss 547.42333984375, Discriminator loss 0.00012672643060795963\n",
      "VAE loss 552.6258544921875, Discriminator loss 0.00012918582069687545\n",
      "VAE loss 550.252197265625, Discriminator loss 0.00011705918586812913\n",
      "VAE loss 546.4472045898438, Discriminator loss 0.00014043221017345786\n",
      "VAE loss 546.8744506835938, Discriminator loss 0.00015347152657341212\n",
      "VAE loss 532.5943603515625, Discriminator loss 0.0001423664652975276\n",
      "VAE loss 554.7847290039062, Discriminator loss 0.00015064595208968967\n",
      "VAE loss 542.884033203125, Discriminator loss 0.00013668343308381736\n",
      "VAE loss 516.5686645507812, Discriminator loss 0.00011949018517043442\n",
      "VAE loss 540.9812622070312, Discriminator loss 0.00013310456415638328\n",
      "VAE loss 522.1968994140625, Discriminator loss 0.0001301830488955602\n",
      "VAE loss 558.0614013671875, Discriminator loss 0.00012129462993470952\n",
      "VAE loss 551.5025024414062, Discriminator loss 0.00012421049177646637\n",
      "VAE loss 521.9880981445312, Discriminator loss 0.00011538442777236924\n",
      "VAE loss 566.7496337890625, Discriminator loss 0.00011914383503608406\n",
      "VAE loss 522.15966796875, Discriminator loss 0.00014971900964155793\n",
      "VAE loss 550.8812866210938, Discriminator loss 0.00011410098522901535\n",
      "VAE loss 527.7513427734375, Discriminator loss 0.00014317143359221518\n",
      "VAE loss 536.90673828125, Discriminator loss 0.0001462381042074412\n",
      "VAE loss 544.8649291992188, Discriminator loss 0.000128268264234066\n",
      "VAE loss 528.2354736328125, Discriminator loss 0.0001299292634939775\n",
      "VAE loss 583.6611328125, Discriminator loss 0.00012070733646396548\n",
      "VAE loss 531.898681640625, Discriminator loss 0.00013615909847430885\n",
      "VAE loss 552.9205932617188, Discriminator loss 0.00012727604189421982\n",
      "VAE loss 540.6444091796875, Discriminator loss 0.00013835664140060544\n",
      "VAE loss 543.6174926757812, Discriminator loss 0.00011693278793245554\n",
      "VAE loss 539.8551025390625, Discriminator loss 0.0001624062715563923\n",
      "VAE loss 561.57666015625, Discriminator loss 0.00010425550863146782\n",
      "VAE loss 526.8384399414062, Discriminator loss 0.0001461009233025834\n",
      "VAE loss 538.00439453125, Discriminator loss 0.0001508526474935934\n",
      "VAE loss 584.0387573242188, Discriminator loss 0.00010663383000064641\n",
      "VAE loss 528.2442016601562, Discriminator loss 0.00013147867866791785\n",
      "VAE loss 526.7884521484375, Discriminator loss 0.00014689912495668977\n",
      "VAE loss 564.5018920898438, Discriminator loss 0.0001189491231343709\n",
      "VAE loss 517.5401000976562, Discriminator loss 0.00012924153998028487\n",
      "VAE loss 522.1988525390625, Discriminator loss 0.00011895760690094903\n",
      "VAE loss 532.0734252929688, Discriminator loss 0.0001312530948780477\n",
      "VAE loss 507.9736022949219, Discriminator loss 0.00012019522546324879\n",
      "VAE loss 578.0303344726562, Discriminator loss 0.00012168563989689574\n",
      "VAE loss 555.454345703125, Discriminator loss 0.00013166636927053332\n",
      "VAE loss 517.7185668945312, Discriminator loss 0.000137261493364349\n",
      "VAE loss 526.188232421875, Discriminator loss 0.00011482752597657964\n",
      "VAE loss 552.1921997070312, Discriminator loss 0.00011835887562483549\n",
      "VAE loss 536.0867309570312, Discriminator loss 0.00011406480916775763\n",
      "VAE loss 538.640869140625, Discriminator loss 0.0001255501847481355\n",
      "VAE loss 556.125, Discriminator loss 0.0001434782607248053\n",
      "VAE loss 528.5545654296875, Discriminator loss 0.00014932923659216613\n",
      "VAE loss 536.5289306640625, Discriminator loss 0.00011168907076353207\n",
      "VAE loss 589.5359497070312, Discriminator loss 0.00012377447274047881\n",
      "VAE loss 586.3969116210938, Discriminator loss 0.00011183854803675786\n",
      "VAE loss 504.50250244140625, Discriminator loss 0.000139449693961069\n",
      "VAE loss 547.7354736328125, Discriminator loss 0.00012571555271279067\n",
      "VAE loss 537.4259033203125, Discriminator loss 0.00012881244765594602\n",
      "VAE loss 549.6024780273438, Discriminator loss 0.00014180662401486188\n",
      "VAE loss 552.2776489257812, Discriminator loss 0.000131550055812113\n",
      "VAE loss 527.7802124023438, Discriminator loss 0.00014811701839789748\n",
      "VAE loss 547.1338500976562, Discriminator loss 0.00016408221563324332\n",
      "VAE loss 550.8233032226562, Discriminator loss 0.00015998959133867174\n",
      "VAE loss 530.02392578125, Discriminator loss 0.00014019719674251974\n",
      "VAE loss 535.3846435546875, Discriminator loss 0.00012827957107219845\n",
      "VAE loss 533.7127075195312, Discriminator loss 0.00010839573224075139\n",
      "VAE loss 537.6834716796875, Discriminator loss 0.00013031582057010382\n",
      "VAE loss 557.6217041015625, Discriminator loss 0.00012185751984361559\n",
      "VAE loss 504.5825500488281, Discriminator loss 0.00014339391782414168\n",
      "VAE loss 525.4962158203125, Discriminator loss 0.00011868229194078594\n",
      "VAE loss 519.3912963867188, Discriminator loss 0.00013320439029484987\n",
      "VAE loss 545.6543579101562, Discriminator loss 0.0001384385395795107\n",
      "VAE loss 529.9071044921875, Discriminator loss 0.00014910924073774368\n",
      "VAE loss 516.23779296875, Discriminator loss 0.00012724909174721688\n",
      "VAE loss 565.4007568359375, Discriminator loss 0.00012334599159657955\n",
      "VAE loss 513.71533203125, Discriminator loss 0.00012896742555312812\n",
      "VAE loss 532.1805419921875, Discriminator loss 0.00012830669584218413\n",
      "VAE loss 537.4055786132812, Discriminator loss 0.0001171660769614391\n",
      "VAE loss 524.0643310546875, Discriminator loss 0.00013673504872713238\n",
      "VAE loss 533.8675537109375, Discriminator loss 0.00015397336392197758\n",
      "VAE loss 534.1445922851562, Discriminator loss 0.00012201458594063297\n",
      "VAE loss 541.6334228515625, Discriminator loss 0.00013236049562692642\n",
      "VAE loss 567.2384643554688, Discriminator loss 0.0001177690428448841\n",
      "VAE loss 535.6221923828125, Discriminator loss 0.00012458728451747447\n",
      "VAE loss 565.6441650390625, Discriminator loss 0.00013014963769819587\n",
      "VAE loss 547.666259765625, Discriminator loss 0.00015699195500928909\n",
      "VAE loss 506.45721435546875, Discriminator loss 0.000125347709399648\n",
      "VAE loss 550.62060546875, Discriminator loss 0.0001622140989638865\n",
      "VAE loss 527.0389404296875, Discriminator loss 0.00012356873776298016\n",
      "VAE loss 543.553466796875, Discriminator loss 0.00014380490756593645\n",
      "VAE loss 522.8814697265625, Discriminator loss 0.00012402940774336457\n",
      "VAE loss 525.396728515625, Discriminator loss 0.00015835299564059824\n",
      "VAE loss 526.06884765625, Discriminator loss 0.00014563380682375282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE loss 538.3797607421875, Discriminator loss 0.0001559929660288617\n",
      "VAE loss 517.8410034179688, Discriminator loss 0.00012813814100809395\n",
      "VAE loss 568.8734741210938, Discriminator loss 0.0001430468837497756\n",
      "VAE loss 522.7560424804688, Discriminator loss 0.00013321448932401836\n",
      "VAE loss 521.6624145507812, Discriminator loss 0.00015303368854802102\n",
      "VAE loss 550.9046630859375, Discriminator loss 0.00013931967259850353\n",
      "VAE loss 508.33544921875, Discriminator loss 0.00013381447934079915\n",
      "VAE loss 546.2183227539062, Discriminator loss 0.00011161640577483922\n",
      "VAE loss 542.7601928710938, Discriminator loss 0.00013857429439667612\n",
      "VAE loss 508.69940185546875, Discriminator loss 0.00013299293641466647\n",
      "VAE loss 528.8076782226562, Discriminator loss 0.00014287720841821283\n",
      "VAE loss 538.0947265625, Discriminator loss 0.00011868993169628084\n",
      "VAE loss 537.97265625, Discriminator loss 0.00014414623728953302\n",
      "VAE loss 532.7929077148438, Discriminator loss 0.00015843850269448012\n",
      "VAE loss 516.7892456054688, Discriminator loss 0.00012482746387831867\n",
      "VAE loss 533.3489379882812, Discriminator loss 0.00015170167898759246\n",
      "VAE loss 544.89990234375, Discriminator loss 0.00011489339522086084\n",
      "VAE loss 547.435546875, Discriminator loss 0.0001746705238474533\n",
      "VAE loss 530.2952270507812, Discriminator loss 0.00011602986342040822\n",
      "VAE loss 542.09814453125, Discriminator loss 0.00012475065886974335\n",
      "VAE loss 542.34130859375, Discriminator loss 0.00013528198178391904\n",
      "VAE loss 527.5682983398438, Discriminator loss 0.00015497248386964202\n",
      "VAE loss 518.18603515625, Discriminator loss 0.00016918346227612346\n",
      "VAE loss 564.5474243164062, Discriminator loss 0.00011939816613448784\n",
      "VAE loss 540.3248901367188, Discriminator loss 0.00012180076009826735\n",
      "VAE loss 545.0979614257812, Discriminator loss 0.0001044955279212445\n",
      "VAE loss 529.1600341796875, Discriminator loss 0.00015216227620840073\n",
      "VAE loss 517.3609619140625, Discriminator loss 0.00013058273179922253\n",
      "VAE loss 517.7136840820312, Discriminator loss 0.00012444454478099942\n",
      "VAE loss 530.1215209960938, Discriminator loss 0.00013505879906006157\n",
      "VAE loss 535.2930297851562, Discriminator loss 0.00012980065366718918\n",
      "VAE loss 541.4265747070312, Discriminator loss 0.00015609496040269732\n",
      "VAE loss 526.2349243164062, Discriminator loss 0.00013426919758785516\n",
      "VAE loss 528.2027587890625, Discriminator loss 0.0001148367955465801\n",
      "VAE loss 563.3734741210938, Discriminator loss 0.00013985857367515564\n",
      "VAE loss 521.9335327148438, Discriminator loss 0.0001470892020734027\n",
      "VAE loss 537.6734619140625, Discriminator loss 0.0001454991870559752\n",
      "VAE loss 531.6060180664062, Discriminator loss 0.00013209745520725846\n",
      "VAE loss 545.6198120117188, Discriminator loss 0.00015042028098832816\n",
      "VAE loss 544.906982421875, Discriminator loss 9.97855604509823e-05\n",
      "VAE loss 544.1494140625, Discriminator loss 0.00011603242455748841\n",
      "VAE loss 514.1561889648438, Discriminator loss 0.00014484381244983524\n",
      "VAE loss 526.7736206054688, Discriminator loss 0.00012945606431458145\n",
      "VAE loss 535.6473388671875, Discriminator loss 0.0001485284446971491\n",
      "VAE loss 510.51422119140625, Discriminator loss 0.00014087925956118852\n",
      "VAE loss 521.9889526367188, Discriminator loss 0.00015063302998896688\n",
      "VAE loss 540.7366333007812, Discriminator loss 0.00013558326463680714\n",
      "VAE loss 527.6511840820312, Discriminator loss 0.00012236852489877492\n",
      "VAE loss 499.1018371582031, Discriminator loss 0.00014091053162701428\n",
      "VAE loss 551.6419677734375, Discriminator loss 0.00013766955817118287\n",
      "VAE loss 565.2151489257812, Discriminator loss 0.00014723316417075694\n",
      "VAE loss 521.6608276367188, Discriminator loss 0.0001321882737101987\n",
      "VAE loss 505.4457092285156, Discriminator loss 0.0001353856932837516\n",
      "VAE loss 529.234130859375, Discriminator loss 0.00013524336100090295\n",
      "VAE loss 531.0843505859375, Discriminator loss 0.00014803321391809732\n",
      "VAE loss 554.6851806640625, Discriminator loss 0.00012252891610842198\n",
      "VAE loss 562.6781005859375, Discriminator loss 0.00013506437244359404\n",
      "VAE loss 535.856201171875, Discriminator loss 0.00012362789129838347\n",
      "VAE loss 514.183349609375, Discriminator loss 0.00014796896721236408\n",
      "VAE loss 514.0415649414062, Discriminator loss 0.00014027809083927423\n",
      "VAE loss 515.8370971679688, Discriminator loss 0.00015842488210182637\n",
      "VAE loss 534.1249389648438, Discriminator loss 0.00011768518015742302\n",
      "VAE loss 553.477294921875, Discriminator loss 0.00015296928177122027\n",
      "VAE loss 546.3291625976562, Discriminator loss 0.00011249034287175164\n",
      "VAE loss 518.8740844726562, Discriminator loss 0.0001601853291504085\n",
      "VAE loss 532.25244140625, Discriminator loss 0.0001310958614340052\n",
      "VAE loss 523.5353393554688, Discriminator loss 0.0001606251171324402\n",
      "VAE loss 553.3977661132812, Discriminator loss 0.0001445388188585639\n",
      "VAE loss 559.4330444335938, Discriminator loss 0.00012165451335022226\n",
      "VAE loss 546.6280517578125, Discriminator loss 0.000139194235089235\n",
      "VAE loss 527.62451171875, Discriminator loss 0.00013722764560952783\n",
      "VAE loss 521.592041015625, Discriminator loss 0.0001464419037802145\n",
      "VAE loss 530.7093505859375, Discriminator loss 0.00012557633453980088\n",
      "VAE loss 535.6878051757812, Discriminator loss 0.0001318478025496006\n",
      "VAE loss 527.8792724609375, Discriminator loss 0.0001244655722985044\n",
      "VAE loss 529.8379516601562, Discriminator loss 0.00015805638395249844\n",
      "VAE loss 539.0869750976562, Discriminator loss 0.00012581105693243444\n",
      "VAE loss 555.43017578125, Discriminator loss 0.00016515454626642168\n",
      "VAE loss 538.4061889648438, Discriminator loss 0.00015493798127863556\n",
      "VAE loss 526.2213134765625, Discriminator loss 0.00011739012552425265\n",
      "VAE loss 513.93017578125, Discriminator loss 0.00013976353511679918\n",
      "VAE loss 517.85009765625, Discriminator loss 0.00012550166866276413\n",
      "VAE loss 530.9341430664062, Discriminator loss 0.00013746469630859792\n",
      "VAE loss 523.6627197265625, Discriminator loss 0.00014482138794846833\n",
      "VAE loss 520.6886596679688, Discriminator loss 0.00014873381587676704\n",
      "VAE loss 503.0927429199219, Discriminator loss 0.00014439679216593504\n",
      "VAE loss 531.8766479492188, Discriminator loss 0.0001563969999551773\n",
      "VAE loss 521.728271484375, Discriminator loss 0.00015833289944566786\n",
      "VAE loss 515.7994995117188, Discriminator loss 0.0001474221789976582\n",
      "VAE loss 532.1185302734375, Discriminator loss 0.00014956739323679358\n",
      "VAE loss 561.9121704101562, Discriminator loss 0.00012275195331312716\n",
      "VAE loss 546.7183837890625, Discriminator loss 0.00015629960398655385\n",
      "VAE loss 521.0459594726562, Discriminator loss 0.00014915036445017904\n",
      "VAE loss 531.07568359375, Discriminator loss 0.0001446134119760245\n",
      "VAE loss 499.6544189453125, Discriminator loss 0.00013513803423848003\n",
      "VAE loss 571.9591674804688, Discriminator loss 0.00013030890841037035\n",
      "VAE loss 524.2855834960938, Discriminator loss 0.0001199229882331565\n",
      "VAE loss 526.07275390625, Discriminator loss 0.00013989576837047935\n",
      "VAE loss 515.5199584960938, Discriminator loss 0.00015430427447427064\n",
      "VAE loss 537.6260375976562, Discriminator loss 0.0001487971458118409\n",
      "VAE loss 545.8649291992188, Discriminator loss 0.00013900143676437438\n",
      "VAE loss 520.9799194335938, Discriminator loss 0.0001647454046178609\n",
      "VAE loss 505.009521484375, Discriminator loss 0.00015731742314528674\n",
      "VAE loss 541.5103149414062, Discriminator loss 0.00015369265747722238\n",
      "VAE loss 513.4675903320312, Discriminator loss 0.00012662789958994836\n",
      "VAE loss 601.3400268554688, Discriminator loss 0.00013244118599686772\n",
      "VAE loss 532.4030151367188, Discriminator loss 0.00013231222692411393\n",
      "VAE loss 527.311767578125, Discriminator loss 0.00015277454804163426\n",
      "VAE loss 522.9515380859375, Discriminator loss 0.00015288760187104344\n",
      "VAE loss 517.613037109375, Discriminator loss 0.0001783424086170271\n",
      "VAE loss 522.816162109375, Discriminator loss 0.00011499267566250637\n",
      "VAE loss 527.3937377929688, Discriminator loss 0.000134320420329459\n",
      "VAE loss 505.6982421875, Discriminator loss 0.00015700640506111085\n",
      "VAE loss 515.1006469726562, Discriminator loss 0.00014738265599589795\n",
      "VAE loss 530.2456665039062, Discriminator loss 0.00012511125532910228\n",
      "VAE loss 534.9967041015625, Discriminator loss 0.00012655832688324153\n",
      "VAE loss 534.4310302734375, Discriminator loss 0.0001472064177505672\n",
      "VAE loss 524.30517578125, Discriminator loss 0.00012494155089370906\n",
      "VAE loss 510.89556884765625, Discriminator loss 0.00011792904115281999\n",
      "VAE loss 550.6617431640625, Discriminator loss 0.00011325955711072311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE loss 535.1820678710938, Discriminator loss 0.00014875901979394257\n",
      "VAE loss 548.68017578125, Discriminator loss 0.00014654544065706432\n",
      "VAE loss 564.7753295898438, Discriminator loss 0.00012587913079187274\n",
      "VAE loss 529.5147094726562, Discriminator loss 0.00015586218796670437\n",
      "VAE loss 532.8271484375, Discriminator loss 0.00015856631216593087\n",
      "VAE loss 533.0596313476562, Discriminator loss 0.0001415151491528377\n",
      "VAE loss 538.02001953125, Discriminator loss 0.00014216647832654417\n",
      "VAE loss 532.8640747070312, Discriminator loss 0.0001665501476963982\n",
      "VAE loss 555.9026489257812, Discriminator loss 0.00015846523456275463\n",
      "VAE loss 531.061767578125, Discriminator loss 0.00014897942310199142\n",
      "VAE loss 522.2068481445312, Discriminator loss 0.0001223772851517424\n",
      "VAE loss 517.1036376953125, Discriminator loss 0.00013930581917520612\n",
      "VAE loss 504.2095947265625, Discriminator loss 0.0001410334516549483\n",
      "VAE loss 538.48876953125, Discriminator loss 0.00015287782298400998\n",
      "VAE loss 511.97283935546875, Discriminator loss 0.00015746090502943844\n",
      "VAE loss 567.9180908203125, Discriminator loss 0.00013731287617702037\n",
      "VAE loss 530.3414306640625, Discriminator loss 0.00015524751506745815\n",
      "VAE loss 576.13671875, Discriminator loss 0.00013698425027541816\n",
      "VAE loss 515.6664428710938, Discriminator loss 0.00016597099602222443\n",
      "VAE loss 536.5152587890625, Discriminator loss 0.0001785475033102557\n",
      "VAE loss 537.0350341796875, Discriminator loss 0.0001638455141801387\n",
      "VAE loss 532.7420654296875, Discriminator loss 0.0001530375302536413\n",
      "VAE loss 495.8061828613281, Discriminator loss 0.0001309241051785648\n",
      "VAE loss 537.1090087890625, Discriminator loss 0.0001623392745386809\n",
      "VAE loss 517.2678833007812, Discriminator loss 0.00013057242904324085\n",
      "VAE loss 519.7510375976562, Discriminator loss 0.00013979480718262494\n",
      "VAE loss 532.5217895507812, Discriminator loss 0.00014092176570557058\n",
      "VAE loss 540.3487548828125, Discriminator loss 0.00013191292237024754\n",
      "VAE loss 560.88134765625, Discriminator loss 0.00012524379417300224\n",
      "VAE loss 532.0912475585938, Discriminator loss 0.00012637344480026513\n",
      "VAE loss 520.7640380859375, Discriminator loss 0.00014122427091933787\n",
      "VAE loss 516.2791137695312, Discriminator loss 0.0001561963726999238\n",
      "VAE loss 528.911376953125, Discriminator loss 0.00013214598584454507\n",
      "VAE loss 522.8070678710938, Discriminator loss 0.0001246850733878091\n",
      "VAE loss 514.0078735351562, Discriminator loss 0.00016562719247303903\n",
      "VAE loss 528.8414306640625, Discriminator loss 0.00015058391727507114\n",
      "VAE loss 504.6195373535156, Discriminator loss 0.00016917027824092656\n",
      "VAE loss 521.035888671875, Discriminator loss 0.00014336869935505092\n",
      "VAE loss 500.268798828125, Discriminator loss 0.00014292322157416493\n",
      "VAE loss 532.5252075195312, Discriminator loss 0.00013683087308891118\n",
      "VAE loss 530.3634643554688, Discriminator loss 0.00014097016537562013\n",
      "VAE loss 529.1626586914062, Discriminator loss 0.000138451112434268\n",
      "VAE loss 547.2577514648438, Discriminator loss 0.00014727844973094761\n",
      "VAE loss 538.1679077148438, Discriminator loss 0.0001643351133679971\n",
      "VAE loss 545.5452880859375, Discriminator loss 0.00015361160330940038\n",
      "VAE loss 523.5994262695312, Discriminator loss 0.00013716096873395145\n",
      "VAE loss 511.3034973144531, Discriminator loss 0.00016908480029087514\n",
      "VAE loss 548.5856323242188, Discriminator loss 0.0001484793028794229\n",
      "VAE loss 582.92431640625, Discriminator loss 0.00012376747326925397\n",
      "VAE loss 524.5383911132812, Discriminator loss 0.00012933214020449668\n",
      "VAE loss 543.6841430664062, Discriminator loss 0.00012762249389197677\n",
      "VAE loss 519.9267578125, Discriminator loss 0.00013560666411649436\n",
      "VAE loss 507.32611083984375, Discriminator loss 0.00012426624016370624\n",
      "VAE loss 520.654541015625, Discriminator loss 0.0001511967129772529\n",
      "VAE loss 524.5137329101562, Discriminator loss 0.0001545421255286783\n",
      "VAE loss 535.7852172851562, Discriminator loss 0.00016489301924593747\n",
      "VAE loss 532.3838500976562, Discriminator loss 0.0001311070373049006\n",
      "VAE loss 546.824951171875, Discriminator loss 0.00014191679656505585\n",
      "VAE loss 533.4903564453125, Discriminator loss 0.00014914599887561053\n",
      "VAE loss 521.8115844726562, Discriminator loss 0.0001520809601061046\n",
      "VAE loss 523.7767944335938, Discriminator loss 0.0001367971271974966\n",
      "VAE loss 547.6694946289062, Discriminator loss 0.00017186219338327646\n",
      "VAE loss 540.4273681640625, Discriminator loss 0.0001428583200322464\n",
      "VAE loss 517.377685546875, Discriminator loss 0.00017157982802018523\n",
      "VAE loss 558.805419921875, Discriminator loss 0.00013671998749487102\n",
      "VAE loss 518.9912109375, Discriminator loss 0.00015196694585029036\n",
      "VAE loss 508.2661437988281, Discriminator loss 0.0001501620135968551\n",
      "VAE loss 510.4035339355469, Discriminator loss 0.00016965690883807838\n",
      "VAE loss 524.9609375, Discriminator loss 0.0001576167851453647\n",
      "VAE loss 516.5831909179688, Discriminator loss 0.000147554324939847\n",
      "VAE loss 523.7938842773438, Discriminator loss 0.00016548259009141475\n",
      "VAE loss 574.5593872070312, Discriminator loss 0.0001445968373445794\n",
      "VAE loss 526.552734375, Discriminator loss 0.00014090427430346608\n",
      "VAE loss 512.93017578125, Discriminator loss 0.0001502044906374067\n",
      "VAE loss 529.8566284179688, Discriminator loss 0.00014697830192744732\n",
      "VAE loss 530.3488159179688, Discriminator loss 0.00013779979781247675\n",
      "VAE loss 520.25244140625, Discriminator loss 0.00014980531705077738\n",
      "VAE loss 540.8740844726562, Discriminator loss 0.00014301080955192447\n",
      "VAE loss 543.6956787109375, Discriminator loss 0.00015195173909887671\n",
      "VAE loss 518.8579711914062, Discriminator loss 0.0001539143268018961\n",
      "VAE loss 535.0386962890625, Discriminator loss 0.0001350549136986956\n",
      "VAE loss 533.22021484375, Discriminator loss 0.0001443451619707048\n",
      "VAE loss 513.9749145507812, Discriminator loss 0.0001684571907389909\n",
      "VAE loss 540.1563720703125, Discriminator loss 0.00016371862147934735\n",
      "VAE loss 529.4412231445312, Discriminator loss 0.0001392778503941372\n",
      "VAE loss 511.9786376953125, Discriminator loss 0.00019578469800762832\n",
      "VAE loss 519.1367797851562, Discriminator loss 0.00013493817823473364\n",
      "VAE loss 533.2142944335938, Discriminator loss 0.00016451830742880702\n",
      "VAE loss 504.2191467285156, Discriminator loss 0.0001548136060591787\n",
      "VAE loss 547.0968017578125, Discriminator loss 0.000131398017401807\n",
      "VAE loss 515.4326782226562, Discriminator loss 0.00016403493646066636\n",
      "VAE loss 511.1068115234375, Discriminator loss 0.00013328893692232668\n",
      "VAE loss 536.8672485351562, Discriminator loss 0.00013990644947625697\n",
      "VAE loss 511.2373046875, Discriminator loss 0.0001276237890124321\n",
      "VAE loss 512.6973876953125, Discriminator loss 0.0001331103267148137\n",
      "VAE loss 518.5204467773438, Discriminator loss 0.00015023666492197663\n",
      "VAE loss 537.9297485351562, Discriminator loss 0.00015349058958236128\n",
      "VAE loss 538.67236328125, Discriminator loss 0.00015732250176370144\n",
      "VAE loss 537.0897216796875, Discriminator loss 0.00015755448839627206\n",
      "VAE loss 508.6675109863281, Discriminator loss 0.00015901854203548282\n",
      "VAE loss 522.244384765625, Discriminator loss 0.00016292175860144198\n",
      "VAE loss 554.0887451171875, Discriminator loss 0.00014680273307021707\n",
      "VAE loss 529.9909057617188, Discriminator loss 0.0001348164223600179\n",
      "VAE loss 527.8104248046875, Discriminator loss 0.00013838379527442157\n",
      "VAE loss 504.271240234375, Discriminator loss 0.00016088978736661375\n",
      "VAE loss 525.8494873046875, Discriminator loss 0.0001522801467217505\n",
      "VAE loss 519.87744140625, Discriminator loss 0.00016953384329099208\n",
      "VAE loss 538.5208740234375, Discriminator loss 0.00018958687724079937\n",
      "VAE loss 501.1189270019531, Discriminator loss 0.00015535442798864096\n",
      "VAE loss 536.3545532226562, Discriminator loss 0.00013270617637317628\n",
      "VAE loss 560.104736328125, Discriminator loss 0.0001248981134267524\n",
      "VAE loss 510.67291259765625, Discriminator loss 0.00014663254842162132\n",
      "VAE loss 512.7799072265625, Discriminator loss 0.00014297099551185966\n",
      "VAE loss 556.1019897460938, Discriminator loss 0.00018115626880899072\n",
      "VAE loss 539.02587890625, Discriminator loss 0.00016139606304932386\n",
      "VAE loss 534.6111450195312, Discriminator loss 0.0001415305450791493\n",
      "VAE loss 532.7816772460938, Discriminator loss 0.00016434493591077626\n",
      "VAE loss 518.0363159179688, Discriminator loss 0.00015504297334700823\n",
      "VAE loss 534.8250122070312, Discriminator loss 0.00012600429181475192\n",
      "VAE loss 525.2215576171875, Discriminator loss 0.00016007311933208257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE loss 509.3583679199219, Discriminator loss 0.00017309258691966534\n",
      "VAE loss 509.7110900878906, Discriminator loss 0.00018188526155427098\n",
      "VAE loss 526.9692993164062, Discriminator loss 0.00015393704234156758\n",
      "VAE loss 526.7998657226562, Discriminator loss 0.00016059608606155962\n",
      "VAE loss 529.0036010742188, Discriminator loss 0.00014781532809138298\n",
      "VAE loss 501.2344665527344, Discriminator loss 0.00012505716586019844\n",
      "VAE loss 536.9705200195312, Discriminator loss 0.00015501634334214032\n",
      "VAE loss 533.8265380859375, Discriminator loss 0.00014959863619878888\n",
      "VAE loss 532.8712158203125, Discriminator loss 0.00018502690363675356\n",
      "VAE loss 543.3826904296875, Discriminator loss 0.00013538679922930896\n",
      "VAE loss 505.2541198730469, Discriminator loss 0.0001578970841364935\n",
      "VAE loss 520.3217163085938, Discriminator loss 0.000148497405461967\n",
      "VAE loss 497.9786682128906, Discriminator loss 0.00013192155165597796\n",
      "VAE loss 541.708984375, Discriminator loss 0.00017087436572182924\n",
      "VAE loss 540.581787109375, Discriminator loss 0.00015589501708745956\n",
      "VAE loss 527.1865844726562, Discriminator loss 0.0001399531465722248\n",
      "VAE loss 488.43243408203125, Discriminator loss 0.00016227694868575782\n",
      "VAE loss 506.3677978515625, Discriminator loss 0.00015113619156181812\n",
      "VAE loss 517.3248291015625, Discriminator loss 0.00016791920643299818\n",
      "VAE loss 550.0493774414062, Discriminator loss 0.00015467983030248433\n",
      "VAE loss 556.0468139648438, Discriminator loss 0.00015013699885457754\n",
      "VAE loss 540.4417724609375, Discriminator loss 0.0001421224878868088\n",
      "VAE loss 524.283935546875, Discriminator loss 0.00016199426318053156\n",
      "VAE loss 519.2605590820312, Discriminator loss 0.00015562698536086828\n",
      "VAE loss 516.1932983398438, Discriminator loss 0.00014806861872784793\n",
      "VAE loss 526.63720703125, Discriminator loss 0.00013787423085886985\n",
      "VAE loss 536.5296020507812, Discriminator loss 0.00015505641931667924\n",
      "VAE loss 520.778564453125, Discriminator loss 0.00017077464144676924\n",
      "VAE loss 533.3129272460938, Discriminator loss 0.00014260601892601699\n",
      "VAE loss 514.1749267578125, Discriminator loss 0.00015131750842556357\n",
      "VAE loss 519.869873046875, Discriminator loss 0.00015285942936316133\n",
      "VAE loss 520.2752685546875, Discriminator loss 0.00014981672575231642\n",
      "VAE loss 531.837646484375, Discriminator loss 0.0001496870390838012\n",
      "VAE loss 506.66510009765625, Discriminator loss 0.0001499324425822124\n",
      "VAE loss 519.5029296875, Discriminator loss 0.00016555811453144997\n",
      "VAE loss 510.85968017578125, Discriminator loss 0.0001437057799194008\n",
      "VAE loss 509.0473327636719, Discriminator loss 0.00015701456868555397\n",
      "VAE loss 512.9400634765625, Discriminator loss 0.0001581803517183289\n",
      "VAE loss 530.8980102539062, Discriminator loss 0.00014874119369778782\n",
      "VAE loss 548.3280639648438, Discriminator loss 0.00013133909669704735\n",
      "VAE loss 523.7578125, Discriminator loss 0.00013292301446199417\n",
      "VAE loss 498.7001647949219, Discriminator loss 0.000148389270179905\n",
      "VAE loss 529.5004272460938, Discriminator loss 0.00015730994346085936\n",
      "VAE loss 522.0448608398438, Discriminator loss 0.00015755192725919187\n",
      "VAE loss 526.1981201171875, Discriminator loss 0.0001249477791134268\n",
      "VAE loss 509.9605712890625, Discriminator loss 0.0001702764566289261\n",
      "VAE loss 502.3592224121094, Discriminator loss 0.00016480694466736168\n",
      "VAE loss 546.8807373046875, Discriminator loss 0.00012933716061525047\n",
      "VAE loss 542.9636840820312, Discriminator loss 0.00013566197594627738\n",
      "VAE loss 538.189453125, Discriminator loss 0.00014817134069744498\n",
      "VAE loss 528.971435546875, Discriminator loss 0.0001478194899391383\n",
      "VAE loss 508.8767395019531, Discriminator loss 0.00017163089069072157\n",
      "VAE loss 525.449462890625, Discriminator loss 0.00018467658082954586\n",
      "VAE loss 500.6855163574219, Discriminator loss 0.00017961043340619653\n",
      "VAE loss 535.6923828125, Discriminator loss 0.00015944548067636788\n",
      "VAE loss 552.4420166015625, Discriminator loss 0.00013378688890952617\n",
      "VAE loss 531.180419921875, Discriminator loss 0.0001363717019557953\n",
      "VAE loss 508.6335754394531, Discriminator loss 0.00015212393191177398\n",
      "VAE loss 505.46148681640625, Discriminator loss 0.00013218139065429568\n",
      "VAE loss 518.5374145507812, Discriminator loss 0.0001531869638711214\n",
      "VAE loss 526.1220092773438, Discriminator loss 0.00013179320376366377\n",
      "VAE loss 522.364990234375, Discriminator loss 0.00019155828340444714\n",
      "VAE loss 525.2747802734375, Discriminator loss 0.00017419183859601617\n",
      "VAE loss 520.1781616210938, Discriminator loss 0.00016107622650451958\n",
      "VAE loss 559.5, Discriminator loss 0.00014485577412415296\n",
      "VAE loss 536.7286987304688, Discriminator loss 0.00013343358295969665\n",
      "VAE loss 495.49090576171875, Discriminator loss 0.00016435956058558077\n",
      "VAE loss 526.565185546875, Discriminator loss 0.00014934946375433356\n",
      "VAE loss 528.3854370117188, Discriminator loss 0.0001388583186781034\n",
      "VAE loss 534.6244506835938, Discriminator loss 0.00013942686200607568\n",
      "VAE loss 508.5308532714844, Discriminator loss 0.00015710594016127288\n",
      "VAE loss 552.51904296875, Discriminator loss 0.00013815243437420577\n",
      "VAE loss 521.8001098632812, Discriminator loss 0.0001368555094813928\n",
      "VAE loss 528.3977661132812, Discriminator loss 0.00014602708688471466\n",
      "VAE loss 545.0674438476562, Discriminator loss 0.0001248672342626378\n",
      "VAE loss 515.0104370117188, Discriminator loss 0.00015694036846980453\n",
      "VAE loss 515.562744140625, Discriminator loss 0.00013567207497544587\n",
      "VAE loss 530.7568359375, Discriminator loss 0.00013837912410963327\n",
      "VAE loss 509.3248291015625, Discriminator loss 0.00016345853509847075\n",
      "VAE loss 526.6951293945312, Discriminator loss 0.00019885937217622995\n",
      "VAE loss 506.9651794433594, Discriminator loss 0.00014020828530192375\n",
      "VAE loss 526.8179321289062, Discriminator loss 0.00013073357695247978\n",
      "VAE loss 527.7804565429688, Discriminator loss 0.0001520780351711437\n",
      "VAE loss 532.96728515625, Discriminator loss 0.00014779044431634247\n",
      "VAE loss 528.9813842773438, Discriminator loss 0.0001499269128544256\n",
      "VAE loss 546.5921020507812, Discriminator loss 0.000149433413753286\n",
      "VAE loss 514.2647094726562, Discriminator loss 0.00014931218174751848\n",
      "VAE loss 518.9000854492188, Discriminator loss 0.0001443260262021795\n",
      "VAE loss 515.6231079101562, Discriminator loss 0.00015151590923778713\n",
      "VAE loss 525.4799194335938, Discriminator loss 0.00015279454237315804\n",
      "VAE loss 517.765380859375, Discriminator loss 0.0001480065839132294\n",
      "VAE loss 540.5967407226562, Discriminator loss 0.0001546418498037383\n",
      "VAE loss 505.0823059082031, Discriminator loss 0.00016216986114159226\n",
      "VAE loss 534.7534790039062, Discriminator loss 0.00015301954408641905\n",
      "VAE loss 529.2750244140625, Discriminator loss 0.0001291303924517706\n",
      "VAE loss 526.31689453125, Discriminator loss 0.00014680395543109626\n",
      "VAE loss 526.9589233398438, Discriminator loss 0.00015684013487771153\n",
      "VAE loss 497.3531188964844, Discriminator loss 0.0001363579649478197\n",
      "VAE loss 540.7224731445312, Discriminator loss 0.00014527761959470809\n",
      "VAE loss 528.0414428710938, Discriminator loss 0.00015533878467977047\n",
      "VAE loss 516.9660034179688, Discriminator loss 0.00015908344357740134\n",
      "VAE loss 537.7818603515625, Discriminator loss 0.0001800496393116191\n",
      "VAE loss 507.8865051269531, Discriminator loss 0.00015906713088043034\n",
      "VAE loss 519.93896484375, Discriminator loss 0.00014847963757347316\n",
      "VAE loss 524.8345336914062, Discriminator loss 0.00014407598064281046\n",
      "VAE loss 512.7854614257812, Discriminator loss 0.00016541742661502212\n",
      "VAE loss 517.5827026367188, Discriminator loss 0.0001568735169712454\n",
      "VAE loss 531.5458374023438, Discriminator loss 0.00015728885773569345\n",
      "VAE loss 529.294921875, Discriminator loss 0.00014696037396788597\n",
      "VAE loss 524.0541381835938, Discriminator loss 0.00017105478036683053\n",
      "VAE loss 520.0819091796875, Discriminator loss 0.00014128501061350107\n",
      "VAE loss 532.0142822265625, Discriminator loss 0.00015293655451387167\n",
      "VAE loss 518.4899291992188, Discriminator loss 0.00015953417459968477\n",
      "VAE loss 512.108154296875, Discriminator loss 0.00015882561274338514\n",
      "VAE loss 494.74066162109375, Discriminator loss 0.00015113901463337243\n",
      "VAE loss 522.8172607421875, Discriminator loss 0.00018210629059467465\n",
      "VAE loss 532.90771484375, Discriminator loss 0.00014300250040832907\n",
      "VAE loss 514.4168090820312, Discriminator loss 0.00015491429076064378\n",
      "VAE loss 522.237548828125, Discriminator loss 0.0001509401627117768\n",
      "VAE loss 511.64154052734375, Discriminator loss 0.00015034213720355183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE loss 507.8023986816406, Discriminator loss 0.00016390986274927855\n",
      "VAE loss 531.9691772460938, Discriminator loss 0.00018334054038859904\n",
      "VAE loss 529.3682861328125, Discriminator loss 0.00015789498866070062\n",
      "VAE loss 525.2808227539062, Discriminator loss 0.00015761010581627488\n",
      "VAE loss 503.7000427246094, Discriminator loss 0.00016698894614819437\n",
      "VAE loss 511.88616943359375, Discriminator loss 0.00014463276602327824\n",
      "VAE loss 498.82879638671875, Discriminator loss 0.00014390924479812384\n",
      "VAE loss 497.83160400390625, Discriminator loss 0.00014998529513832182\n",
      "VAE loss 521.6765747070312, Discriminator loss 0.00014046095020603389\n",
      "VAE loss 517.7830810546875, Discriminator loss 0.00014988119073677808\n",
      "VAE loss 561.05029296875, Discriminator loss 0.00013413421402219683\n",
      "VAE loss 500.7754821777344, Discriminator loss 0.0001630795595701784\n",
      "VAE loss 521.8494873046875, Discriminator loss 0.0001672748039709404\n",
      "VAE loss 522.9144897460938, Discriminator loss 0.00013803622277919203\n",
      "VAE loss 519.3978881835938, Discriminator loss 0.00014533473586197942\n",
      "VAE loss 518.8026733398438, Discriminator loss 0.000175465815118514\n",
      "VAE loss 529.8506469726562, Discriminator loss 0.0001605483121238649\n",
      "VAE loss 512.00732421875, Discriminator loss 0.0001299417926929891\n",
      "VAE loss 532.7857055664062, Discriminator loss 0.00014317604654934257\n",
      "VAE loss 551.2169799804688, Discriminator loss 0.00015057469136081636\n",
      "VAE loss 520.449462890625, Discriminator loss 0.0001671189966145903\n",
      "VAE loss 541.3280639648438, Discriminator loss 0.00015101031749509275\n",
      "VAE loss 508.4171142578125, Discriminator loss 0.00015099949087016284\n",
      "VAE loss 520.2341918945312, Discriminator loss 0.0001469030830776319\n",
      "VAE loss 520.060791015625, Discriminator loss 0.00013144811964593828\n",
      "VAE loss 528.4932861328125, Discriminator loss 0.00014689168892800808\n",
      "VAE loss 501.8885498046875, Discriminator loss 0.0001463777298340574\n",
      "VAE loss 527.6209716796875, Discriminator loss 0.00015460344729945064\n",
      "VAE loss 513.484619140625, Discriminator loss 0.00015394231013488024\n",
      "VAE loss 536.0148315429688, Discriminator loss 0.00016119990323204547\n",
      "VAE loss 519.7810668945312, Discriminator loss 0.0001278500712942332\n",
      "VAE loss 511.33685302734375, Discriminator loss 0.00013940826465841383\n",
      "VAE loss 540.20947265625, Discriminator loss 0.0001402294437866658\n",
      "VAE loss 519.22705078125, Discriminator loss 0.00014533162175212055\n",
      "VAE loss 540.2791748046875, Discriminator loss 0.0001406244991812855\n",
      "VAE loss 496.36370849609375, Discriminator loss 0.00014117675891611725\n",
      "VAE loss 517.814697265625, Discriminator loss 0.0001570045278640464\n",
      "VAE loss 515.5090942382812, Discriminator loss 0.00015868264017626643\n",
      "VAE loss 525.2810668945312, Discriminator loss 0.0001448659459128976\n",
      "VAE loss 507.41455078125, Discriminator loss 0.00016511903959326446\n",
      "VAE loss 502.13031005859375, Discriminator loss 0.00018475097022019327\n",
      "VAE loss 557.769775390625, Discriminator loss 0.0001697363768471405\n",
      "VAE loss 523.9902954101562, Discriminator loss 0.00013744454190600663\n",
      "VAE loss 519.459716796875, Discriminator loss 0.00015110117965377867\n",
      "VAE loss 540.430419921875, Discriminator loss 0.00015649101987946779\n",
      "VAE loss 512.713134765625, Discriminator loss 0.00016031139239203185\n",
      "VAE loss 500.9347839355469, Discriminator loss 0.0001705556205706671\n",
      "VAE loss 518.2804565429688, Discriminator loss 0.00015343110135290772\n",
      "VAE loss 534.4850463867188, Discriminator loss 0.00013983933604322374\n",
      "VAE loss 518.6357421875, Discriminator loss 0.00018949669902212918\n",
      "VAE loss 553.918701171875, Discriminator loss 0.00014234177069738507\n",
      "VAE loss 513.9632568359375, Discriminator loss 0.0001623228599783033\n",
      "VAE loss 513.4716186523438, Discriminator loss 0.00016055681044235826\n",
      "VAE loss 511.41094970703125, Discriminator loss 0.00018404630827717483\n",
      "VAE loss 517.3842163085938, Discriminator loss 0.00014489222667180002\n",
      "VAE loss 504.9527587890625, Discriminator loss 0.0001781367463991046\n",
      "VAE loss 545.3702392578125, Discriminator loss 0.00016305888129863888\n",
      "VAE loss 517.5571899414062, Discriminator loss 0.00012715162301901728\n",
      "VAE loss 515.6043701171875, Discriminator loss 0.00016562739619985223\n",
      "VAE loss 507.3891296386719, Discriminator loss 0.00017401101649738848\n",
      "VAE loss 522.676513671875, Discriminator loss 0.00013205646246206015\n",
      "VAE loss 524.3570556640625, Discriminator loss 0.00016708024486433715\n",
      "VAE loss 521.9471435546875, Discriminator loss 0.0001473755983170122\n",
      "VAE loss 495.00128173828125, Discriminator loss 0.00014901634131092578\n",
      "VAE loss 515.8641967773438, Discriminator loss 0.00015761562099214643\n",
      "VAE loss 517.5567626953125, Discriminator loss 0.00014399750216398388\n",
      "VAE loss 524.2355346679688, Discriminator loss 0.00015815447841305286\n",
      "VAE loss 506.130126953125, Discriminator loss 0.00016531808068975806\n",
      "VAE loss 519.83837890625, Discriminator loss 0.00015500266454182565\n",
      "VAE loss 528.319580078125, Discriminator loss 0.00018579140305519104\n",
      "VAE loss 504.4989013671875, Discriminator loss 0.0001449763949494809\n",
      "VAE loss 505.66656494140625, Discriminator loss 0.00014774924784433097\n",
      "VAE loss 511.4895324707031, Discriminator loss 0.00014127303438726813\n",
      "VAE loss 537.2125854492188, Discriminator loss 0.0001473954616812989\n",
      "VAE loss 548.3377685546875, Discriminator loss 0.00015630492998752743\n",
      "VAE loss 554.3302001953125, Discriminator loss 0.0001886816171463579\n",
      "VAE loss 522.6437377929688, Discriminator loss 0.00016019120812416077\n",
      "VAE loss 511.197021484375, Discriminator loss 0.00012726837303489447\n",
      "VAE loss 526.2658081054688, Discriminator loss 0.00013944614329375327\n",
      "VAE loss 522.2376708984375, Discriminator loss 0.0001503249950474128\n",
      "VAE loss 541.3619384765625, Discriminator loss 0.00017254478007089347\n",
      "VAE loss 500.2649230957031, Discriminator loss 0.00018210105190519243\n",
      "VAE loss 521.567626953125, Discriminator loss 0.0001467280526412651\n",
      "VAE loss 520.5194091796875, Discriminator loss 0.00013576739002019167\n",
      "VAE loss 520.5154418945312, Discriminator loss 0.00018398005340714008\n",
      "VAE loss 522.1780395507812, Discriminator loss 0.00014010306040290743\n",
      "VAE loss 506.3571472167969, Discriminator loss 0.0001687712356215343\n",
      "VAE loss 523.5877685546875, Discriminator loss 0.00015938100113999099\n",
      "VAE loss 516.57763671875, Discriminator loss 0.00016278507246170193\n",
      "VAE loss 533.2195434570312, Discriminator loss 0.000174657950992696\n",
      "VAE loss 504.3473205566406, Discriminator loss 0.00016560929361730814\n",
      "VAE loss 517.0427856445312, Discriminator loss 0.00015935112605802715\n",
      "VAE loss 503.7252502441406, Discriminator loss 0.0001666571042733267\n",
      "VAE loss 529.0420532226562, Discriminator loss 0.00015598800382576883\n",
      "VAE loss 508.7774353027344, Discriminator loss 0.00016728456830605865\n",
      "VAE loss 538.678466796875, Discriminator loss 0.00012771665933541954\n",
      "VAE loss 515.00341796875, Discriminator loss 0.0001525136030977592\n",
      "VAE loss 527.9761352539062, Discriminator loss 0.00014741983613930643\n",
      "VAE loss 493.076904296875, Discriminator loss 0.0001749454386299476\n",
      "VAE loss 525.39404296875, Discriminator loss 0.00016078329645097256\n",
      "VAE loss 510.17926025390625, Discriminator loss 0.00014795605966355652\n",
      "VAE loss 511.08221435546875, Discriminator loss 0.00016018813767004758\n",
      "VAE loss 521.9921875, Discriminator loss 0.00015622490900568664\n",
      "VAE loss 502.16162109375, Discriminator loss 0.00016097811749204993\n",
      "VAE loss 523.3192138671875, Discriminator loss 0.0001597641676198691\n",
      "VAE loss 510.3598327636719, Discriminator loss 0.00016356802370864898\n",
      "VAE loss 542.5062255859375, Discriminator loss 0.00015991987311281264\n",
      "VAE loss 530.4530639648438, Discriminator loss 0.0001450468844268471\n",
      "VAE loss 514.6259765625, Discriminator loss 0.0001685235765762627\n",
      "VAE loss 533.7079467773438, Discriminator loss 0.0001424267393304035\n",
      "VAE loss 518.0994262695312, Discriminator loss 0.000149209241499193\n",
      "VAE loss 510.1826171875, Discriminator loss 0.0001565141137689352\n",
      "VAE loss 509.2680969238281, Discriminator loss 0.00015027247718535364\n",
      "VAE loss 503.2412414550781, Discriminator loss 0.00013272359501570463\n",
      "VAE loss 512.9832153320312, Discriminator loss 0.00016543385572731495\n",
      "VAE loss 526.0219116210938, Discriminator loss 0.0001705958420643583\n",
      "VAE loss 528.8441772460938, Discriminator loss 0.00013208626478444785\n",
      "VAE loss 515.3344116210938, Discriminator loss 0.0001629204343771562\n",
      "VAE loss 490.9200744628906, Discriminator loss 0.00015971796528901905\n",
      "VAE loss 495.53985595703125, Discriminator loss 0.00015962152974680066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE loss 513.2195434570312, Discriminator loss 0.0001546186686027795\n",
      "VAE loss 531.0538940429688, Discriminator loss 0.00016985443653538823\n",
      "VAE loss 523.1806030273438, Discriminator loss 0.00016430785763077438\n",
      "VAE loss 503.2920837402344, Discriminator loss 0.00016337160195689648\n",
      "VAE loss 532.2060546875, Discriminator loss 0.0001453462173230946\n",
      "VAE loss 527.031005859375, Discriminator loss 0.00015331136819440871\n",
      "VAE loss 529.3211059570312, Discriminator loss 0.00014009667211212218\n",
      "VAE loss 516.733154296875, Discriminator loss 0.0001686164760030806\n",
      "VAE loss 499.5291442871094, Discriminator loss 0.00019738933769986033\n",
      "VAE loss 507.3721618652344, Discriminator loss 0.0001548037980683148\n",
      "VAE loss 521.7100830078125, Discriminator loss 0.0001414749276591465\n",
      "VAE loss 502.6408386230469, Discriminator loss 0.0001724678440950811\n",
      "VAE loss 523.0045776367188, Discriminator loss 0.00014429357543122023\n",
      "VAE loss 529.9843139648438, Discriminator loss 0.00016255081573035568\n",
      "VAE loss 494.3109436035156, Discriminator loss 0.000152685766806826\n",
      "VAE loss 520.8843994140625, Discriminator loss 0.00016118366329465061\n",
      "VAE loss 524.5236206054688, Discriminator loss 0.00016687106108292937\n",
      "VAE loss 507.1640930175781, Discriminator loss 0.00016844170750118792\n",
      "VAE loss 511.5042724609375, Discriminator loss 0.00016639442765153944\n",
      "VAE loss 541.3994750976562, Discriminator loss 0.00015348118904512376\n",
      "VAE loss 513.3092651367188, Discriminator loss 0.00013797478459309787\n",
      "VAE loss 510.2986755371094, Discriminator loss 0.00018798274686560035\n",
      "VAE loss 521.3543701171875, Discriminator loss 0.00017170498904306442\n",
      "VAE loss 526.9011840820312, Discriminator loss 0.0001479583152104169\n",
      "VAE loss 511.2323303222656, Discriminator loss 0.0001479379425290972\n",
      "VAE loss 516.9852905273438, Discriminator loss 0.00018266112601850182\n",
      "VAE loss 555.4137573242188, Discriminator loss 0.00011824396642623469\n",
      "VAE loss 525.8692016601562, Discriminator loss 0.000155989735503681\n",
      "VAE loss 511.5374755859375, Discriminator loss 0.00015130483370739967\n",
      "VAE loss 515.5526123046875, Discriminator loss 0.00015757157234475017\n",
      "VAE loss 511.3542785644531, Discriminator loss 0.00019119812350254506\n",
      "VAE loss 512.4844360351562, Discriminator loss 0.00015062540478538722\n",
      "VAE loss 498.2397155761719, Discriminator loss 0.00015742273535579443\n",
      "VAE loss 505.4129638671875, Discriminator loss 0.00014706101501360536\n",
      "VAE loss 514.7554321289062, Discriminator loss 0.00016128079732879996\n",
      "VAE loss 511.2151184082031, Discriminator loss 0.00017982647113967687\n",
      "VAE loss 524.4254760742188, Discriminator loss 0.000139424970257096\n",
      "VAE loss 499.5708923339844, Discriminator loss 0.00017023835971485823\n",
      "VAE loss 549.5568237304688, Discriminator loss 0.0001588331797393039\n",
      "VAE loss 526.2763671875, Discriminator loss 0.00015123296179808676\n",
      "VAE loss 500.20269775390625, Discriminator loss 0.00016590960149187595\n",
      "VAE loss 512.018798828125, Discriminator loss 0.00013746420154348016\n",
      "VAE loss 549.0446166992188, Discriminator loss 0.0001397502637701109\n",
      "VAE loss 534.8427124023438, Discriminator loss 0.00012883591989520937\n",
      "VAE loss 516.152099609375, Discriminator loss 0.0001620068505872041\n",
      "VAE loss 516.43310546875, Discriminator loss 0.00015381505363620818\n",
      "VAE loss 502.4265441894531, Discriminator loss 0.00014176972035784274\n",
      "VAE loss 513.6439819335938, Discriminator loss 0.00013649268657900393\n",
      "VAE loss 495.8735656738281, Discriminator loss 0.0001585007703397423\n",
      "VAE loss 515.2037353515625, Discriminator loss 0.00014689496310893446\n",
      "VAE loss 520.256103515625, Discriminator loss 0.0001333626132691279\n",
      "VAE loss 517.022705078125, Discriminator loss 0.00015334402269218117\n",
      "VAE loss 500.5274963378906, Discriminator loss 0.0001820755423977971\n",
      "VAE loss 518.8057861328125, Discriminator loss 0.00014833215391263366\n",
      "VAE loss 533.4613647460938, Discriminator loss 0.0001307146594626829\n",
      "VAE loss 510.53521728515625, Discriminator loss 0.00014468010340351611\n",
      "VAE loss 522.678466796875, Discriminator loss 0.00015922298189252615\n",
      "VAE loss 511.69195556640625, Discriminator loss 0.00014009572623763233\n",
      "VAE loss 509.90509033203125, Discriminator loss 0.00015925649495329708\n",
      "VAE loss 496.0700378417969, Discriminator loss 0.00018444628221914172\n",
      "VAE loss 550.6589965820312, Discriminator loss 0.0001647416065679863\n",
      "VAE loss 539.129638671875, Discriminator loss 0.00014904157433193177\n",
      "VAE loss 518.0264282226562, Discriminator loss 0.00013970391592010856\n",
      "VAE loss 529.74658203125, Discriminator loss 0.0001428485702490434\n",
      "VAE loss 515.2005004882812, Discriminator loss 0.00016983223031274974\n",
      "VAE loss 506.6597900390625, Discriminator loss 0.0001536705094622448\n",
      "VAE loss 540.0751342773438, Discriminator loss 0.00014721859770361334\n",
      "VAE loss 521.728515625, Discriminator loss 0.00016236754890996963\n",
      "VAE loss 523.4906005859375, Discriminator loss 0.00011085725418524817\n",
      "VAE loss 520.71826171875, Discriminator loss 0.0001699798449408263\n",
      "VAE loss 516.5348510742188, Discriminator loss 0.00015285713016055524\n",
      "VAE loss 514.3905639648438, Discriminator loss 0.0001528343273093924\n",
      "VAE loss 525.3911743164062, Discriminator loss 0.00014427691348828375\n",
      "VAE loss 497.12225341796875, Discriminator loss 0.00016693402722012252\n",
      "VAE loss 496.35870361328125, Discriminator loss 0.0001440279302187264\n",
      "VAE loss 515.6427612304688, Discriminator loss 0.00015509632066823542\n",
      "VAE loss 514.587646484375, Discriminator loss 0.00013945961836725473\n",
      "VAE loss 514.6453857421875, Discriminator loss 0.00016201929247472435\n",
      "VAE loss 532.1113891601562, Discriminator loss 0.00014369327982421964\n",
      "VAE loss 517.4599609375, Discriminator loss 0.0001338157308055088\n",
      "VAE loss 523.5060424804688, Discriminator loss 0.00016941969806794077\n",
      "VAE loss 503.4953918457031, Discriminator loss 0.00014462041144724935\n",
      "VAE loss 495.3802490234375, Discriminator loss 0.00015108330990187824\n",
      "VAE loss 515.505126953125, Discriminator loss 0.0001593506895005703\n",
      "VAE loss 523.4376220703125, Discriminator loss 0.0001270577631657943\n",
      "VAE loss 519.42529296875, Discriminator loss 0.00015963459736667573\n",
      "VAE loss 505.28131103515625, Discriminator loss 0.0001642189163248986\n",
      "VAE loss 530.9544067382812, Discriminator loss 0.0001525398256490007\n",
      "VAE loss 511.211181640625, Discriminator loss 0.00016168293950613588\n",
      "VAE loss 538.4676513671875, Discriminator loss 0.00014100951375439763\n",
      "\n",
      "[2020-05-06 00:06:24,500] INFO - scvi.inference.inference | Training is still in warming up phase. If your applications rely on the posterior quality, consider training for more epochs or reducing the kl warmup.\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50 if n_epochs_all is None else n_epochs_all\n",
    "\n",
    "# use_cuda to use GPU\n",
    "use_cuda = False\n",
    "\n",
    "scvi_posterior, scvi_latent = compute_scvi_latent(\n",
    "    adata_original, n_epochs=n_epochs, n_latent=10, use_cuda=use_cuda\n",
    ")\n",
    "adata.obsm[\"vae\"] = scvi_latent\n",
    "\n",
    "# store scvi imputed expression\n",
    "scale = scvi_posterior.get_sample_scale()\n",
    "for _ in range(9):\n",
    "    scale += scvi_posterior.get_sample_scale()\n",
    "scale /= 10\n",
    "\n",
    "for gene, gene_scale in zip(adata.var.index, np.squeeze(scale).T):\n",
    "    adata.obs[\"scale_\" + gene] = gene_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transforming to str index.\n",
      "... storing 'cell_types' as categorical\n"
     ]
    }
   ],
   "source": [
    "# fullfactor = factortrainer.create_posterior(factortrainer.model, gene_dataset, indices=np.arange(len(gene_dataset)))\n",
    "save_dir = os.path.join(save_path, \"pmbc3kfactor\")\n",
    "if not os.path.exists(save_dir):\n",
    "    scvi_posterior.save_posterior(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_index = [0]\n",
    "\n",
    "\n",
    "def get_mean_var(prior):\n",
    "    meanmax = np.ones([1,10])*-float('inf')\n",
    "    meanmin = np.ones([1, 10]) * float('inf')\n",
    "    means = []\n",
    "    variance = []\n",
    "    for tensors in prior:\n",
    "        sample_batch, local_l_mean, local_l_var, batch_index, label = tensors\n",
    "        qz_m, qz_v, z = prior.model.z_encoder(sample_batch, label) \n",
    "        maxvector = np.max(qz_m.detach().numpy(), axis=0)\n",
    "        minvector = np.max(qz_m.detach().numpy(), axis=0)\n",
    "        meanmax = np.maximum(maxvector, meanmax)\n",
    "        meanmin = np.minimum(minvector, meanmin)\n",
    "        \n",
    "    \n",
    "    return meanmax, meanmin\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-inf, -inf, -inf]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones([1,3]) * -float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17.26130676  5.47344589  6.06225967  7.79175329  7.88382101  7.84673357\n",
      "   8.2051239   9.08538628  6.00363874  8.07103348]]\n",
      "[[43.31898499 16.09925079 13.20314884 33.23301315 21.3868885  35.55144882\n",
      "  17.23241615 16.56752205 26.86712265 25.21201134]]\n"
     ]
    }
   ],
   "source": [
    "meanmax, meanin = get_mean_var(scvi_posterior)\n",
    "print(meanin)\n",
    "print(meanmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def updatelatent(prior, fidx, newval):\n",
    "   \n",
    "    for tensors in prior:\n",
    "        sample_batch, local_l_mean, local_l_var, batch_index, label = tensors\n",
    "        qz_m, qz_v, z = prior.model.z_encoder(sample_batch, label) \n",
    "        qz_m[:, fidx] = newval \n",
    "       \n",
    "    return qz_m, qz_v, z\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def testql(prior, fidx, newval):\n",
    "   \n",
    "    for tensors in prior:\n",
    "        sample_batch, local_l_mean, local_l_var, batch_index, label = tensors\n",
    "        qz_m, qz_v, z = prior.model.l_encoder(sample_batch) \n",
    "        \n",
    "        print(\"ql_m :{}, ql_v: {}\".format(qz_m.shape, qz_v.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ql_m :torch.Size([128, 1]), ql_v: torch.Size([128, 1])\n",
      "ql_m :torch.Size([128, 1]), ql_v: torch.Size([128, 1])\n",
      "ql_m :torch.Size([128, 1]), ql_v: torch.Size([128, 1])\n",
      "ql_m :torch.Size([128, 1]), ql_v: torch.Size([128, 1])\n",
      "ql_m :torch.Size([128, 1]), ql_v: torch.Size([128, 1])\n",
      "ql_m :torch.Size([128, 1]), ql_v: torch.Size([128, 1])\n",
      "ql_m :torch.Size([128, 1]), ql_v: torch.Size([128, 1])\n",
      "ql_m :torch.Size([128, 1]), ql_v: torch.Size([128, 1])\n",
      "ql_m :torch.Size([128, 1]), ql_v: torch.Size([128, 1])\n",
      "ql_m :torch.Size([128, 1]), ql_v: torch.Size([128, 1])\n",
      "ql_m :torch.Size([128, 1]), ql_v: torch.Size([128, 1])\n",
      "ql_m :torch.Size([128, 1]), ql_v: torch.Size([128, 1])\n",
      "ql_m :torch.Size([128, 1]), ql_v: torch.Size([128, 1])\n",
      "ql_m :torch.Size([128, 1]), ql_v: torch.Size([128, 1])\n",
      "ql_m :torch.Size([128, 1]), ql_v: torch.Size([128, 1])\n",
      "ql_m :torch.Size([128, 1]), ql_v: torch.Size([128, 1])\n",
      "ql_m :torch.Size([128, 1]), ql_v: torch.Size([128, 1])\n",
      "ql_m :torch.Size([128, 1]), ql_v: torch.Size([128, 1])\n",
      "ql_m :torch.Size([128, 1]), ql_v: torch.Size([128, 1])\n",
      "ql_m :torch.Size([128, 1]), ql_v: torch.Size([128, 1])\n",
      "ql_m :torch.Size([78, 1]), ql_v: torch.Size([78, 1])\n"
     ]
    }
   ],
   "source": [
    " testql(scvi_posterior, 0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([78, 10])\n"
     ]
    }
   ],
   "source": [
    "qz_m, qz, z = updatelatent(scvi_posterior, 0, 0.0)\n",
    "\n",
    "print(qz_m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qz_m[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal\n",
    "\n",
    "interpolation = torch.arange(-10, 50, 10)\n",
    "@torch.no_grad()\n",
    "def updatelatent(posterior, fidx, newval):\n",
    "    result = []\n",
    "    n_samples = 1\n",
    "    for tensors in prior:\n",
    "        sample_batch, local_l_mean, local_l_var, batch_index, label = tensors\n",
    "        qz_m, qz_v, z = posterior.model.z_encoder(sample_batch, label) \n",
    "        ql_m, ql_v, library = posterior.model.l_encoder(sample_batch)\n",
    "\n",
    "        qz_m[:, fidx] = newval \n",
    "        \n",
    "        qz_m = qz_m.unsqueeze(0).expand((n_samples, qz_m.size(0), qz_m.size(1)))\n",
    "        qz_v = qz_v.unsqueeze(0).expand((n_samples, qz_v.size(0), qz_v.size(1)))\n",
    "        # when z is normal, untran_z == z\n",
    "        untran_z = Normal(qz_m, qz_v.sqrt()).sample()\n",
    "        z = posterior.model.z_encoder.z_transformation(untran_z)\n",
    "        ql_m = ql_m.unsqueeze(0).expand((n_samples, ql_m.size(0), ql_m.size(1)))\n",
    "        ql_v = ql_v.unsqueeze(0).expand((n_samples, ql_v.size(0), ql_v.size(1)))\n",
    "        library = Normal(ql_m, ql_v.sqrt()).sample()\n",
    "        \n",
    "        \n",
    "        px_scale, px_r, px_rate, px_dropout = posterior.model.decoder(\n",
    "            prior.model.dispersion, z, library, None, label\n",
    "        )\n",
    "        \n",
    "       \n",
    "        result.append(px_scale.view(px_scale.shape[1:]))\n",
    "    \n",
    "    result = torch.cat(result, 0)\n",
    "    result = result.detach().numpy()\n",
    "    return result\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = updatelatent(scvi_posterior, 0, -10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2638, 1838)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.3354772e-06, 6.8981221e-09, 2.2916866e-09, ..., 7.1821828e-12,\n",
       "       1.2195823e-06, 1.6921551e-06], dtype=float32)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.17146961, -0.28081226, -0.04667677, ..., -0.09826882,\n",
       "       -0.20909512, -0.5312033 ], dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.X[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
