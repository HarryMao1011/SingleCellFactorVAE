{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scvi.dataset import CortexDataset, RetinaDataset\n",
    "from scvi.models import VAE\n",
    "from scvi.inference import UnsupervisedTrainer, load_posterior\n",
    "from scvi import set_seed\n",
    "import torch\n",
    "\n",
    "# Control UMAP numba warnings\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "save_path = \"data/\"\n",
    "\n",
    "# Sets torch and numpy random seeds, run after all scvi imports\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from scvi.dataset import GeneExpressionDataset\n",
    "from typing import Tuple, Dict, Union\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal, kl_divergence as kl\n",
    "\n",
    "from scvi.models.distributions import (\n",
    "    ZeroInflatedNegativeBinomial,\n",
    "    NegativeBinomial,\n",
    "    Poisson,\n",
    ")\n",
    "from scvi.models.modules import Encoder, DecoderSCVI, LinearDecoderSCVI\n",
    "from scvi.models.utils import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        print(\"z_dim is {}\".format(z_dim))\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(z_dim, 200),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Linear(200, 200),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Linear(200, 200),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Linear(200, 200),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            # nn.Linear(200, 200),\n",
    "            # nn.LeakyReLU(0.2, True),\n",
    "            nn.Linear(200, 2),\n",
    "        )\n",
    "        self.weight_init()\n",
    "\n",
    "    def weight_init(self, mode='normal'):\n",
    "        if mode == 'kaiming':\n",
    "            initializer = kaiming_init\n",
    "        elif mode == 'normal':\n",
    "            initializer = normal_init\n",
    "\n",
    "        for block in self._modules:\n",
    "            for m in self._modules[block]:\n",
    "                initializer(m)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming_init(m):\n",
    "    if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "    elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "        m.weight.data.fill_(1)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "def normal_init(m):\n",
    "    if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "        init.normal_(m.weight, 0, 0.02)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "    elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "        m.weight.data.fill_(1)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorVAE(VAE):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input: int,\n",
    "        n_batch: int = 0,\n",
    "        n_labels: int = 0,\n",
    "        n_hidden: int = 128,\n",
    "        n_latent: int = 10,\n",
    "        n_layers: int = 1,\n",
    "        dropout_rate: float = 0.1,\n",
    "        dispersion: str = \"gene\",\n",
    "        log_variational: bool = True,\n",
    "        reconstruction_loss: str = \"zinb\",\n",
    "        latent_distribution: str = \"normal\"\n",
    "    ):\n",
    "        super().__init__(n_input, n_batch, n_labels, n_hidden, n_latent, n_layers,\n",
    "                         dropout_rate, dispersion, log_variational,reconstruction_loss,\n",
    "                         latent_distribution)\n",
    "    def forward(\n",
    "        self, x, local_l_mean, local_l_var, batch_index=None, y=None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\" Returns the reconstruction loss and the KL divergences\n",
    "        :param x: tensor of values with shape (batch_size, n_input)\n",
    "        :param local_l_mean: tensor of means of the prior distribution of latent variable l\n",
    "         with shape (batch_size, 1)\n",
    "        :param local_l_var: tensor of variancess of the prior distribution of latent variable l\n",
    "         with shape (batch_size, 1)\n",
    "        :param batch_index: array that indicates which batch the cells belong to with shape ``batch_size``\n",
    "        :param y: tensor of cell-types labels with shape (batch_size, n_labels)\n",
    "        :return: the reconstruction loss and the Kullback divergences\n",
    "        \"\"\"\n",
    "        # Parameters for z latent distribution\n",
    "        outputs = self.inference(x, batch_index, y)\n",
    "        qz_m = outputs[\"qz_m\"]\n",
    "        qz_v = outputs[\"qz_v\"]\n",
    "        ql_m = outputs[\"ql_m\"]\n",
    "        ql_v = outputs[\"ql_v\"]\n",
    "        px_rate = outputs[\"px_rate\"]\n",
    "        px_r = outputs[\"px_r\"]\n",
    "        px_dropout = outputs[\"px_dropout\"]\n",
    "\n",
    "        # KL Divergence\n",
    "        mean = torch.zeros_like(qz_m)\n",
    "        scale = torch.ones_like(qz_v)\n",
    "\n",
    "        kl_divergence_z = kl(Normal(qz_m, torch.sqrt(qz_v)), Normal(mean, scale)).sum(\n",
    "            dim=1\n",
    "        )\n",
    "        kl_divergence_l = kl(\n",
    "            Normal(ql_m, torch.sqrt(ql_v)),\n",
    "            Normal(local_l_mean, torch.sqrt(local_l_var)),\n",
    "        ).sum(dim=1)\n",
    "        kl_divergence = kl_divergence_z\n",
    "\n",
    "        reconst_loss = self.get_reconstruction_loss(x, px_rate, px_r, px_dropout)\n",
    "\n",
    "        return reconst_loss + kl_divergence_l, kl_divergence, 0.0\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class factorTrain(UnsupervisedTrainer):\n",
    "    \"\"\"The VariationalInference class for the unsupervised training of an autoencoder.\n",
    "    Args:\n",
    "        :model: A model instance from class ``VAE``, ``VAEC``, ``SCANVI``, ``AutoZIVAE``\n",
    "        :discriminator: A discriminator\n",
    "        :gene_dataset: A gene_dataset instance like ``CortexDataset()``\n",
    "        :train_size: The train size, either a float between 0 and 1 or an integer for the number of training samples\n",
    "         to use Default: ``0.8``.\n",
    "        :test_size: The test size, either a float between 0 and 1 or an integer for the number of training samples\n",
    "         to use Default: ``None``, which is equivalent to data not in the train set. If ``train_size`` and ``test_size``\n",
    "         do not add to 1 or the length of the dataset then the remaining samples are added to a ``validation_set``.\n",
    "        Two parameters can help control the training KL annealing\n",
    "        If your applications rely on the posterior quality,\n",
    "        (i.e. differential expression, batch effect removal), ensure the number of total\n",
    "        epochs (or iterations) exceed the number of epochs (or iterations) used for KL warmup\n",
    "\n",
    "        :n_epochs_kl_warmup: Number of epochs for linear warmup of KL(q(z|x)||p(z)) term. After `n_epochs_kl_warmup`,\n",
    "                the training objective is the ELBO. This might be used to prevent inactivity of latent units, and/or to\n",
    "                improve clustering of latent space, as a long warmup turns the model into something more of an autoencoder.\n",
    "                Be aware that large datasets should avoid this mode and rely on n_iter_kl_warmup. If this parameter is not\n",
    "                None, then it overrides any choice of `n_iter_kl_warmup`.\n",
    "        :n_iter_kl_warmup: Number of iterations for warmup (useful for bigger datasets)\n",
    "            int(128*5000/400) is a good default value.\n",
    "        :normalize_loss: A boolean determining whether the loss is divided by the total number of samples used for\n",
    "            training. In particular, when the global KL divergence is equal to 0 and the division is performed, the loss\n",
    "            for a minibatchis is equal to the average of reconstruction losses and KL divergences on the minibatch.\n",
    "            Default: ``None``, which is equivalent to setting False when the model is an instance from class\n",
    "            ``AutoZIVAE`` and True otherwise.\n",
    "        :\\*\\*kwargs: Other keywords arguments from the general Trainer class.\n",
    "        int(400.0 * 5000 / 128.0)\n",
    "    Examples:\n",
    "\n",
    "    \"\"\"\n",
    "    default_metrics_to_monitor = [\"elbo\"]\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model,\n",
    "            gene_dataset: GeneExpressionDataset,\n",
    "            train_size: Union[int, float] = 0.8,\n",
    "            test_size: Union[int, float] = None,\n",
    "            n_iter_kl_warmup: Union[int, None] = None,\n",
    "            n_epochs_kl_warmup: Union[int, None] = 400,\n",
    "            normalize_loss: bool = None,\n",
    "            lr_D: float = 1e-4,\n",
    "            beta1_D: float = 0.5,\n",
    "            beta2_D: float = 0.9,\n",
    "            **kwargs):\n",
    "        super().__init__(model, gene_dataset, train_size, test_size, n_iter_kl_warmup,\n",
    "                         n_epochs_kl_warmup, normalize_loss, **kwargs)\n",
    "        self.D = Discriminator(self.model.n_latent)\n",
    "        print(\"model n latent {}\".format(model.n_latent))\n",
    "        self.lr_D = lr_D\n",
    "        self.beta1_D = beta1_D\n",
    "        self.beta2_D = beta2_D\n",
    "        self.optim_D = optim.Adam(self.D.parameters(), lr=self.lr_D,\n",
    "                                  betas=(self.beta1_D, self.beta2_D))\n",
    "        self.D_loss = 0.0\n",
    "        (\n",
    "            self.train_set,\n",
    "            self.test_set,\n",
    "            self.validation_set,\n",
    "        ) = self.train_test_validation(model, gene_dataset, train_size, test_size)\n",
    "        self.train_set.to_monitor = [\"elbo\"]\n",
    "        self.test_set.to_monitor = [\"elbo\"]\n",
    "        self.validation_set.to_monitor = [\"elbo\"]\n",
    "        self.n_samples = len(self.train_set.indices)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = std.data.new(std.size()).normal_()\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def permute_dims(self, z):\n",
    "        assert z.dim() == 2\n",
    "\n",
    "        B, _ = z.size()\n",
    "        perm_z = []\n",
    "        for z_j in z.split(1, 1):\n",
    "            perm = torch.randperm(B).to(z.device)\n",
    "            perm_z_j = z_j[perm]\n",
    "            perm_z.append(perm_z_j)\n",
    "\n",
    "        return torch.cat(perm_z, 1)\n",
    "\n",
    "    def loss(self, tensors):\n",
    "\n",
    "        sample_batch, local_l_mean, local_l_var, batch_index, y = tensors\n",
    "        reconst_loss, kl_divergence_local, kl_divergence_global = self.model(\n",
    "            sample_batch, local_l_mean, local_l_var, batch_index, y\n",
    "        )\n",
    "        loss = (\n",
    "                self.n_samples\n",
    "                * torch.mean(reconst_loss + self.kl_weight * kl_divergence_local)\n",
    "                + kl_divergence_global\n",
    "        )\n",
    "\n",
    "        # print(\"n_batch is {}\".format(sample_batch.shape))\n",
    "        z_prime = self.model.get_latents(sample_batch, y)[0]\n",
    "        # print(\"zPrime shape is {}\".format(z_prime.shape))\n",
    "\n",
    "        # qz_m = outputs[\"qz_m\"]\n",
    "        # qz_v = outputs[\"qz_v\"]\n",
    "        #\n",
    "        # z_prime = self.reparametrize(qz_m, qz_v)\n",
    "        z_pperm = self.permute_dims(z_prime).detach()\n",
    "        D_z_pperm = self.D(z_pperm)\n",
    "        D_z = self.D(z_prime)\n",
    "        ones = torch.ones(sample_batch.shape[0], dtype=torch.long)\n",
    "        zeros = torch.zeros(sample_batch.shape[0], dtype=torch.long)\n",
    "\n",
    "        D_tc_loss = 0.5 * (F.cross_entropy(D_z, zeros) + F.cross_entropy(D_z_pperm, ones))\n",
    "\n",
    "        if self.normalize_loss:\n",
    "            loss = loss / self.n_samples\n",
    "            D_tc_loss = D_tc_loss / self.n_samples\n",
    "\n",
    "        return loss, D_tc_loss\n",
    "\n",
    "    def on_training_loop(self, tensors_list):\n",
    "        loss, D_loss = self.loss(*tensors_list)\n",
    "        self.current_loss, self.D_loss = loss, D_loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.optim_D.zero_grad()\n",
    "        D_loss.backward()\n",
    "        self.optim_D.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-04-25 12:45:36,380] INFO - scvi.dataset.dataset | File /Users/ham112/Documents/GenomicVAE/data/expression.bin already downloaded\n",
      "[2020-04-25 12:45:36,381] INFO - scvi.dataset.cortex | Loading Cortex data\n",
      "[2020-04-25 12:45:53,372] INFO - scvi.dataset.cortex | Finished preprocessing Cortex data\n",
      "[2020-04-25 12:45:54,825] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n",
      "[2020-04-25 12:45:54,827] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-04-25 12:45:57,421] INFO - scvi.dataset.dataset | Downsampling from 19972 to 1000 genes\n",
      "[2020-04-25 12:45:57,584] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-04-25 12:45:57,622] INFO - scvi.dataset.dataset | Filtering non-expressing cells.\n",
      "[2020-04-25 12:45:57,731] INFO - scvi.dataset.dataset | Computing the library size for the new data\n",
      "[2020-04-25 12:45:57,737] INFO - scvi.dataset.dataset | Downsampled from 3005 to 3005 cells\n",
      "[2020-04-25 12:45:57,746] INFO - scvi.dataset.dataset | Making gene names lower case\n"
     ]
    }
   ],
   "source": [
    "### test the API\n",
    "\n",
    "gene_dataset = CortexDataset(save_path=save_path, total_genes=None)\n",
    "gene_dataset.subsample_genes(1000, mode=\"variance\")\n",
    "gene_dataset.make_gene_names_lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-04-23 15:49:51,422] INFO - scvi.inference.inference | KL warmup phase exceeds overall training phaseIf your applications rely on the posterior quality, consider training for more epochs or reducing the kl warmup.\n",
      "[2020-04-23 15:49:51,423] INFO - scvi.inference.inference | KL warmup for 400 epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b9d76a56414b2a883ba337ea21a774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training', max=2.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2020-04-23 15:49:55,158] INFO - scvi.inference.inference | Training is still in warming up phase. If your applications rely on the posterior quality, consider training for more epochs or reducing the kl warmup.\n"
     ]
    }
   ],
   "source": [
    "## setting up the hyper parameters\n",
    "n_epochs = 2\n",
    "lr = 1e-3\n",
    "use_cuda = False\n",
    "## setting up the training model\n",
    "vae = VAE(gene_dataset.nb_genes)\n",
    "trainer = UnsupervisedTrainer(\n",
    "    vae,\n",
    "    gene_dataset,\n",
    "    train_size=0.90,\n",
    "    use_cuda=use_cuda,\n",
    "    frequency=5,\n",
    ")\n",
    "trainer.train(n_epochs=n_epochs, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a35a92050>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "elbo_train_set = trainer.history[\"elbo_train_set\"]\n",
    "elbo_test_set = trainer.history[\"elbo_test_set\"]\n",
    "x = np.linspace(0, 2, (len(elbo_train_set)))\n",
    "plt.plot(x, elbo_train_set, label=\"train\")\n",
    "plt.plot(x, elbo_test_set, label=\"test\")\n",
    "plt.ylim(1500, 3000)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_dim is 10\n",
      "model n latent 10\n"
     ]
    }
   ],
   "source": [
    "## Test Factor VAE\n",
    "\n",
    "factorVAE = FactorVAE(gene_dataset.nb_genes)\n",
    "n_epochs = 2\n",
    "lr = 1e-3\n",
    "use_cuda = False\n",
    "factortrainer = factorTrain(\n",
    "    factorVAE,\n",
    "    gene_dataset,\n",
    "    train_size=0.90,\n",
    "    use_cuda=use_cuda,\n",
    "    frequency=5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(factortrainer) is UnsupervisedTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-04-25 12:46:07,198] INFO - scvi.inference.inference | KL warmup phase exceeds overall training phaseIf your applications rely on the posterior quality, consider training for more epochs or reducing the kl warmup.\n",
      "[2020-04-25 12:46:07,199] INFO - scvi.inference.inference | KL warmup for 400 epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "710dbd527b6242fdb4467a90cef80b07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training', max=2.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2020-04-25 12:46:11,748] INFO - scvi.inference.inference | Training is still in warming up phase. If your applications rely on the posterior quality, consider training for more epochs or reducing the kl warmup.\n"
     ]
    }
   ],
   "source": [
    "factortrainer.train(n_epochs=n_epochs, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
